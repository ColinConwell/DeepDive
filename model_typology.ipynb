{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Torchvision Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision_dictlist = [\n",
    "    \n",
    "    {'model': 'alexnet', 'model_display_name': 'AlexNet', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'vgg11', 'model_display_name': 'VGG11', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg13', 'model_display_name': 'VGG13', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg16', 'model_display_name': 'VGG16', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg19', 'model_display_name': 'VGG19', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg11_bn', 'model_display_name': 'VGG11-BatchNorm', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg13_bn', 'model_display_name': 'VGG13-BatchNorm', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg16_bn', 'model_display_name': 'VGG16-BatchNorm', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg19_bn', 'model_display_name': 'VGG19-BatchNorm', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'resnet18', 'model_display_name': 'ResNet18', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'resnet34', 'model_display_name': 'ResNet34', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnet50', 'model_display_name': 'ResNet50', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnet101', 'model_display_name': 'ResNet101', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnet152', 'model_display_name': 'ResNet152', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'squeezenet1_0', 'model_display_name': 'SqueezeNet1.0', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'squeezenet1_1', 'model_display_name': 'SqueezeNet1.1', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet121', 'model_display_name': 'DenseNet121', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet161', 'model_display_name': 'DenseNet161', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet169', 'model_display_name': 'DenseNet169', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet201', 'model_display_name': 'DenseNet201', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'googlenet', 'model_display_name': 'GoogleNet', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'shufflenet_v2_x0_5', 'model_display_name': 'ShuffleNet-V2-x0.5', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    " \n",
    "    {'model': 'shufflenet_v2_x1_0', 'model_display_name': 'ShuffleNet-V2-x1.0', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mobilenet_v2', 'model_display_name': 'MobileNet-V2', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnext50_32x4d', 'model_display_name': 'ResNext50-32x4D', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnext101_32x8d', 'model_display_name': 'ResNext50-32x8D', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'wide_resnet50_2', 'model_display_name': 'Wide-ResNet50', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'wide_resnet101_2', 'model_display_name': 'Wide-ResNet101', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mnasnet0_5', 'model_display_name': 'MNASNet0.5', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mnasnet1_0', 'model_display_name': 'MNASNet1.0', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "]\n",
    "\n",
    "\n",
    "train_type_text = {\n",
    "    'imagenet': 'image classification',\n",
    "    'inception': 'image classification',\n",
    "    'video': 'video classification',\n",
    "    'segmentation': 'image segmentation',\n",
    "    'detection': 'keypoint detection'\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "    'kinetics400': 'Kinetics400',\n",
    "    'coco2017': 'CoCo2017',\n",
    "}\n",
    "\n",
    "torchvision_dictlist_ = copy(torchvision_dictlist)\n",
    "for i, dict_i in enumerate(torchvision_dictlist):\n",
    "    dict_i_random = copy(dict_i)\n",
    "    dict_i_random['train_data'] = None\n",
    "    dict_i_random['train_type'] = 'random'\n",
    "    dict_i_random['task_cluster'] = 'Random'\n",
    "    torchvision_dictlist_.append(dict_i_random)\n",
    "torchvision_dictlist = torchvision_dictlist_\n",
    "    \n",
    "    \n",
    "for i, dict_i in enumerate(torchvision_dictlist):\n",
    "    dict_i = torchvision_dictlist[i]\n",
    "    if dict_i['train_type'] != 'random':\n",
    "        dict_i['description'] = '{} trained on {} with the {} dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                               train_type_text[dict_i['train_type']],\n",
    "                                                                               train_data_text[dict_i['train_data']])\n",
    "    if dict_i['train_type'] == 'random':\n",
    "        dict_i['description'] = '{} randomly initialized, with no training.'.format(dict_i['model_display_name'])\n",
    "    \n",
    "torchvision_df = pd.DataFrame(torchvision_dictlist)\n",
    "torchvision_df['task_cluster'] = 'Semantic'\n",
    "torchvision_df['model_class'] = 'Convolutional'\n",
    "torchvision_df['model_source'] = 'torchvision'\n",
    "torchvision_df['model_source_url'] = 'pytorch.org/docs/stable/torchvision/models.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vgg13_bn</td>\n",
       "      <td>VGG13-BatchNorm</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>VGG13-BatchNorm trained on image classificatio...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vgg16_bn</td>\n",
       "      <td>VGG16-BatchNorm</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>VGG16-BatchNorm trained on image classificatio...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>densenet201</td>\n",
       "      <td>DenseNet201</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>random</td>\n",
       "      <td>None</td>\n",
       "      <td>DenseNet201 randomly initialized, with no trai...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model model_display_name model_type train_type train_data  \\\n",
       "6      vgg13_bn    VGG13-BatchNorm   imagenet   imagenet   imagenet   \n",
       "7      vgg16_bn    VGG16-BatchNorm   imagenet   imagenet   imagenet   \n",
       "49  densenet201        DenseNet201   imagenet     random       None   \n",
       "\n",
       "                                          description task_cluster  \\\n",
       "6   VGG13-BatchNorm trained on image classificatio...     Semantic   \n",
       "7   VGG16-BatchNorm trained on image classificatio...     Semantic   \n",
       "49  DenseNet201 randomly initialized, with no trai...     Semantic   \n",
       "\n",
       "      model_class model_source  \\\n",
       "6   Convolutional  torchvision   \n",
       "7   Convolutional  torchvision   \n",
       "49  Convolutional  torchvision   \n",
       "\n",
       "                                   model_source_url  \n",
       "6   pytorch.org/docs/stable/torchvision/models.html  \n",
       "7   pytorch.org/docs/stable/torchvision/models.html  \n",
       "49  pytorch.org/docs/stable/torchvision/models.html  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Taskonomy Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskonomy_dictlist = [\n",
    "    \n",
    "    {'model': 'autoencoding', 'model_display_name': 'Autoencoder', 'task_cluster': '2D',\n",
    "        'description': 'Image compression and decompression'},\n",
    "    \n",
    "    {'model': 'class_object', 'model_display_name': 'Object Classification', 'task_cluster': 'Semantic',\n",
    "        'description': '1000-way object classification (via knowledge distillation from ImageNet).'},\n",
    "    \n",
    "    {'model': 'class_scene', 'model_display_name': 'Scene Classification', 'task_cluster': 'Semantic',\n",
    "        'description': 'Scene Classification (via knowledge distillation from MIT Places).'},\n",
    "    \n",
    "    {'model': 'curvature', 'model_display_name': 'Curvatures', 'task_cluster': '3D',\n",
    "        'description': 'Magnitude of 3D principal curvatures'},\n",
    "    \n",
    "    {'model': 'colorization', 'model_display_name': 'Colorization', 'task_cluster': None,\n",
    "        'description': 'Colorizing input grayscale images.'},\n",
    "    \n",
    "    {'model': 'denoising', 'model_display_name': 'Denoising', 'task_cluster': 'Other',\n",
    "        'description': 'Uncorrupted version of corrupted image.'},\n",
    "    \n",
    "    {'model': 'depth_euclidean', 'model_display_name': 'Euclidean Depth', 'task_cluster': '3D',\n",
    "        'description': 'Depth estimation'},\n",
    "    \n",
    "    {'model': 'depth_zbuffer', 'model_display_name': 'Z-Buffer Depth', 'task_cluster': '3D',\n",
    "        'description': 'Depth estimation.'},\n",
    "    \n",
    "    {'model': 'edge_occlusion', 'model_display_name': 'Occlusion Edges', 'task_cluster': '3D',\n",
    "        'description': 'Edges which include parts of the scene.'},\n",
    "    \n",
    "    {'model': 'edge_texture', 'model_display_name': 'Texture Edges', 'task_cluster': '2D',\n",
    "        'description': 'Edges computed from RGB only (texture edges).'},\n",
    "    \n",
    "    {'model': 'egomotion', 'model_display_name': 'Egomotion', 'task_cluster': 'Geometric',\n",
    "        'description': 'Odometry (camera poses) given three input images.'},\n",
    "    \n",
    "    {'model': 'fixated_pose', 'model_display_name': 'Camera Pose (Fixated)', 'task_cluster': 'Geometric',\n",
    "        'description': 'Relative camera pose with matching optical centers.'},\n",
    "    \n",
    "    {'model': 'inpainting', 'model_display_name': 'Inpainting', 'task_cluster': '2D',\n",
    "        'description': 'Filling in masked center of image.'},\n",
    "    \n",
    "    {'model': 'jigsaw', 'model_display_name': 'Jigsaw', 'task_cluster': 'Geometric',\n",
    "        'description': 'Putting scrambled image pieces back together.'},\n",
    "    \n",
    "    {'model': 'keypoints2d', 'model_display_name': '2D Keypoints', 'task_cluster': '2D',\n",
    "        'description': 'Keypoint estimation from RGB-only (texture features).'},\n",
    "    \n",
    "    {'model': 'keypoints3d', 'model_display_name': '3D Keypoints', 'task_cluster': '3D',\n",
    "        'description': '3D Keypoint estimation from underlying scene 3D.'},\n",
    "    \n",
    "    {'model': 'nonfixated_pose', 'model_display_name': 'Camera Pose (Nonfixated)', 'task_cluster': 'Geometric',\n",
    "        'description': 'Relative camera pose with distinct optical centers.'},\n",
    "    \n",
    "    {'model': 'normal', 'model_display_name': 'Surface Normals', 'task_cluster': '3D',\n",
    "        'description': 'Pixel-wise surface normals.'},\n",
    "    \n",
    "    {'model': 'point_matching', 'model_display_name': 'Point Matching', 'task_cluster': 'Geometric',\n",
    "        'description': 'Classifying if centers of two images match or not.'},\n",
    "    \n",
    "    {'model': 'reshading', 'model_display_name': 'Reshading', 'task_cluster': '3D',\n",
    "        'description': 'Reshading with new lighting placed at camera location.'},\n",
    "    \n",
    "    {'model': 'room_layout', 'model_display_name': 'Room Layout', 'task_cluster': 'Geometric',\n",
    "        'description': 'Orientation and aspect ratio of cubic room layout.'},\n",
    "    \n",
    "    {'model': 'segment_semantic', 'model_display_name': 'Semantic Segmentation', 'task_cluster': 'Semantic',\n",
    "        'description': 'Pixel-wise semantic labeling (via knowledge distillation from MS COCO).'},\n",
    "    \n",
    "    {'model': 'segment_unsup25d', 'model_display_name': 'Unsupervised 2.5D Segmentation', 'task_cluster': '3D',\n",
    "        'description': 'Segmentation (graph cut approximation) on RGB-D-Normals-Curvature image.'},\n",
    "    \n",
    "    {'model': 'segment_unsup2d', 'model_display_name': 'Unsupervised 2D Segmentation', 'task_cluster': '2D',\n",
    "        'description': 'Segmentation (graph cut approximation) on RGB.'},\n",
    "    \n",
    "    {'model': 'vanishing_point', 'model_display_name': 'Vanishing Point', 'task_cluster': 'Geometric',\n",
    "        'description': 'Three Manhattan-world vanishing points.'}\n",
    "]\n",
    "\n",
    "taskonomy_df = pd.DataFrame(taskonomy_dictlist)\n",
    "problematic_models = ['colorization']\n",
    "taskonomy_df = taskonomy_df[~taskonomy_df.model.isin(problematic_models)]\n",
    "taskonomy_df['model_type'] = 'taskonomy'\n",
    "taskonomy_df['train_type'] = 'taskonomy'\n",
    "taskonomy_df['train_data'] = 'taskonomy'\n",
    "taskonomy_df['model_class'] = 'Convolutional'\n",
    "\n",
    "task_random = {'model': 'random_weights', 'model_display_name': 'Random Weights', \n",
    "               'task_cluster': 'Random', 'description': 'Taskonomy architecture randomly initialized.',\n",
    "               'model_type': 'taskonomy', 'train_type': 'taskonomy', 'train_data': None}\n",
    "\n",
    "task_random_df = pd.DataFrame(task_random,  index = [taskonomy_df.shape[0] + 1])\n",
    "\n",
    "taskonomy_df = pd.concat([taskonomy_df, task_random_df])\n",
    "\n",
    "taskonomy_df['model_source'] = 'taskonomy'\n",
    "taskonomy_df['model_source_url'] = 'github.com/alexsax/midlevel-reps/tree/visualpriors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>description</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>segment_unsup2d</td>\n",
       "      <td>Unsupervised 2D Segmentation</td>\n",
       "      <td>2D</td>\n",
       "      <td>Segmentation (graph cut approximation) on RGB.</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>class_object</td>\n",
       "      <td>Object Classification</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>1000-way object classification (via knowledge ...</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>segment_semantic</td>\n",
       "      <td>Semantic Segmentation</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>Pixel-wise semantic labeling (via knowledge di...</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model            model_display_name task_cluster  \\\n",
       "23   segment_unsup2d  Unsupervised 2D Segmentation           2D   \n",
       "1       class_object         Object Classification     Semantic   \n",
       "21  segment_semantic         Semantic Segmentation     Semantic   \n",
       "\n",
       "                                          description model_type train_type  \\\n",
       "23     Segmentation (graph cut approximation) on RGB.  taskonomy  taskonomy   \n",
       "1   1000-way object classification (via knowledge ...  taskonomy  taskonomy   \n",
       "21  Pixel-wise semantic labeling (via knowledge di...  taskonomy  taskonomy   \n",
       "\n",
       "   train_data    model_class model_source  \\\n",
       "23  taskonomy  Convolutional    taskonomy   \n",
       "1   taskonomy  Convolutional    taskonomy   \n",
       "21  taskonomy  Convolutional    taskonomy   \n",
       "\n",
       "                                     model_source_url  \n",
       "23  github.com/alexsax/midlevel-reps/tree/visualpr...  \n",
       "1   github.com/alexsax/midlevel-reps/tree/visualpr...  \n",
       "21  github.com/alexsax/midlevel-reps/tree/visualpr...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taskonomy_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Timm (Imagenet) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_dictlist = [\n",
    "    \n",
    "    {'model': 'beit_base_patch16_224', 'model_display_name': 'BEiT-S-P16', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'beit_large_patch16_224', 'model_display_name': 'BEiT-L-P16', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'cait_s24_224', 'model_display_name': 'CaiT-S24', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'cait_xxs24_224', 'model_display_name': 'CaiT-XXS24', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'coat_lite_tiny', 'model_display_name': 'CoaT-Lite-Tiny', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'coat_lite_small', 'model_display_name': 'CoaT-Lite-Small', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'coat_mini', 'model_display_name': 'CoaT-Mini', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'convit_base', 'model_display_name': 'ConViT-B', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'convit_small', 'model_display_name': 'ConViT-S', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'convit_tiny', 'model_display_name': 'ConViT-T', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'crossvit_base_240', 'model_display_name': 'CrossViT-B', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'deit_base_patch16_224', 'model_display_name': 'DeiT-B-P16-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'deit_small_patch16_224', 'model_display_name': 'DeiT-S-P16-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'deit_tiny_patch16_224', 'model_display_name': 'DeiT-T-P16-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'cspdarknet53', 'model_display_name': 'CSP-DarkNet53', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'cspresnet50', 'model_display_name': 'CSP-ResNet50', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'dla34', 'model_display_name': 'DLA34', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'dla169', 'model_display_name': 'DLA169', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'eca_nfnet_l0', 'model_display_name': 'ECA-NFNeT-L0', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'eca_nfnet_l1', 'model_display_name': 'ECA-NFNeT-L1', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'ecaresnet50d', 'model_display_name': 'ECA-Resnet50-D', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'ecaresnet101d', 'model_display_name': 'ECA-Resnet101-D', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'efficientnet_b1', 'model_display_name': 'EfficientNet-B1', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'efficientnet_b3', 'model_display_name': 'EfficientNet-B3', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'gcresnet50t', 'model_display_name': 'GCResNet50T', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'efficientnetv2_rw_s', 'model_display_name': 'EfficientNet-V2-S', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'fbnetc_100', 'model_display_name': 'FBNetC100', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'gernet_l', 'model_display_name': 'GerNet-L', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'gernet_s', 'model_display_name': 'GerNet-S', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'ghostnet_100', 'model_display_name': 'GhostNet100', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'gmixer_24_224', 'model_display_name': 'GMixer-24', 'model_class': 'MLP-Mixer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'gmlp_s16_224', 'model_display_name': 'GMLP-S16', 'model_class': 'MLP-Mixer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'halonet26t', 'model_display_name': 'HaloNet-26T', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'hardcorenas_a', 'model_display_name': 'HardCoreNAS-A', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'hardcorenas_f', 'model_display_name': 'HardCoreNAS-F', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'hrnet_w18', 'model_display_name': 'HRNet-W18', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'hrnet_w64', 'model_display_name': 'HRNet-W64', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'jx_nest_base', 'model_display_name': 'JX-NesT-Base', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'jx_nest_small', 'model_display_name': 'JX-NesT-Small', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'jx_nest_tiny', 'model_display_name': 'JX-NesT-Small', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'lambda_resnet26t', 'model_display_name': 'Lambda-ResNet-26T', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'levit_128', 'model_display_name': 'LeViT128', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'levit_256', 'model_display_name': 'LeViT256', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'inception_resnet_v2', 'model_display_name': 'Inception-Resnet-V2', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'inception_v3', 'model_display_name': 'Inception-V3', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'inception_v4', 'model_display_name': 'Inception-V4', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'inception_v4', 'model_display_name': 'Inception-V4', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mixer_b16_224', 'model_display_name': 'MLP-Mixer-B16', 'model_class': 'MLP-Mixer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'mixer_l16_224', 'model_display_name': 'MLP-Mixer-L16', 'model_class': 'MLP-Mixer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mixnet_l', 'model_display_name': 'MixNet-L', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mixnet_s', 'model_display_name': 'MixNet-S', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mnasnet_100', 'model_display_name': 'MNASNet100', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mnasnet_100', 'model_display_name': 'MNASNet100', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "        \n",
    "    {'model': 'mobilenetv3_large_100', 'model_display_name': 'MobileNet-V3-Large', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mobilenetv3_rw', 'model_display_name': 'MobileNet-V3', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'nasnetalarge', 'model_display_name': 'NASNet-A-Large', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'nf_resnet50', 'model_display_name': 'NF-ResNet50', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'nfnet_l0', 'model_display_name': 'NF-Net-L0', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'pit_b_224', 'model_display_name': 'PiT-B-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'pit_s_224', 'model_display_name': 'PiT-S-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'pit_ti_224', 'model_display_name': 'PiT-T-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'pnasnet5large', 'model_display_name': 'PNASNet-5-Large', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'regnetx_064', 'model_display_name': 'RegNetX-64', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'regnety_064', 'model_display_name': 'RegNetY-64', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'repvgg_b1', 'model_display_name': 'RepVGG-B1', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'repvgg_b1g4', 'model_display_name': 'RepVGG-B1G4', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'res2net50_26w_4s', 'model_display_name': 'Res2Net50-26W-4S', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resmlp_12_224', 'model_display_name': 'ResMLP-12', 'model_class': 'MLP-Mixer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resmlp_24_224', 'model_display_name': 'ResMLP-24', 'model_class': 'MLP-Mixer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resmlp_36_224', 'model_display_name': 'ResMLP-36', 'model_class': 'MLP-Mixer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resmlp_big_24_224', 'model_display_name': 'ResMLP-Big-24', 'model_class': 'MLP-Mixer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnest50d', 'model_display_name': 'ResNest50D', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnetrs50', 'model_display_name': 'ResNetRS50', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'rexnet_100', 'model_display_name': 'RexNet100', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'selecsls60', 'model_display_name': 'SelecSLS60', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'semnasnet_100', 'model_display_name': 'SemNASNet100', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'seresnet50', 'model_display_name': 'SEResNet50', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'seresnext50_32x4d', 'model_display_name': 'SEResNext50-32x4D', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'skresnet18', 'model_display_name': 'SKResNet18', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'skresnext50_32x4d', 'model_display_name': 'SKResNext50-32x4D', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'spnasnet_100', 'model_display_name': 'SPNasNet100', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'swin_base_patch4_window7_224', 'model_display_name': 'Swin-B-P4-W7', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'swin_large_patch4_window7_224', 'model_display_name': 'Swin-L-P4-W7', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'swin_small_patch4_window7_224', 'model_display_name': 'Swin-S-P4-W7', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'swin_tiny_patch4_window7_224', 'model_display_name': 'Swin-T-P4-W7', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'tnt_s_patch16_224', 'model_display_name': 'TnT-P16-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'tresnet_l', 'model_display_name': 'TResNet-L', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'twins_pcpvt_base', 'model_display_name': 'Twins-PCPVT-B', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'twins_svt_base', 'model_display_name': 'Twins-SVT-B', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'visformer_small', 'model_display_name': 'Visformer', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vit_large_patch16_224', 'model_display_name': 'ViT-L-P16', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vit_small_patch16_224', 'model_display_name': 'ViT-S-P16', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vit_tiny_patch16_224', 'model_display_name': 'ViT-T-P16', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vit_base_patch16_224', 'model_display_name': 'ViT-B-P16', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vit_base_patch32_224', 'model_display_name': 'ViT-B-P32', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vit_small_patch32_224', 'model_display_name': 'ViT-S-P32', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vit_base_patch16_sam_224', 'model_display_name': 'ViT-B-P16-SAM', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vit_small_patch32_sam_224', 'model_display_name': 'ViT-S-P32-SAM', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'xception', 'model_display_name': 'XCeption', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'xception65', 'model_display_name': 'XCeption65', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'xcit_nano_12_p16_224', 'model_display_name': 'XCIT-Nano-P16', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'xcit_small_12_p16_224', 'model_display_name': 'XCIT-S-P16', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'xcit_large_24_p16_224', 'model_display_name': 'XCIT-L-P16', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'xcit_nano_12_p8_224', 'model_display_name': 'XCIT-Nano-P8', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'xcit_nano_12_p8_224', 'model_display_name': 'XCIT-S-P8', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'xcit_large_24_p16_224', 'model_display_name': 'XCIT-Nano-P8', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "]\n",
    "\n",
    "train_type_text = {\n",
    "    'imagenet': 'image classification',\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "}\n",
    "\n",
    "timm_dictlist_ = copy(timm_dictlist)\n",
    "for i, dict_i in enumerate(timm_dictlist):\n",
    "    dict_i_random = copy(dict_i)\n",
    "    dict_i_random['train_data'] = None\n",
    "    dict_i_random['train_type'] = 'random'\n",
    "    dict_i_random['task_cluster'] = 'Random'\n",
    "    timm_dictlist_.append(dict_i_random)\n",
    "timm_dictlist = timm_dictlist_\n",
    "    \n",
    "    \n",
    "for i, dict_i in enumerate(timm_dictlist):\n",
    "    dict_i = timm_dictlist[i]\n",
    "    if dict_i['train_type'] != 'random':\n",
    "        dict_i['description'] = '{} trained on {} with the {} dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                               train_type_text[dict_i['train_type']],\n",
    "                                                                               train_data_text[dict_i['train_data']])\n",
    "    if dict_i['train_type'] == 'random':\n",
    "        dict_i['description'] = '{} randomly initialized, with no training.'.format(dict_i['model_display_name'])\n",
    "    \n",
    "timm_df = pd.DataFrame(timm_dictlist)\n",
    "timm_df['task_cluster'] = 'Semantic'\n",
    "timm_df['model_source'] = 'timm'\n",
    "timm_df['model_source_url'] = 'https://github.com/rwightman/pytorch-image-models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>halonet26t</td>\n",
       "      <td>HaloNet-26T</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>random</td>\n",
       "      <td>None</td>\n",
       "      <td>HaloNet-26T randomly initialized, with no trai...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>visformer_small</td>\n",
       "      <td>Visformer</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>random</td>\n",
       "      <td>None</td>\n",
       "      <td>Visformer randomly initialized, with no training.</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>vit_small_patch32_224</td>\n",
       "      <td>ViT-S-P32</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>ViT-S-P32 trained on image classification with...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model model_display_name    model_class model_type  \\\n",
       "138             halonet26t        HaloNet-26T  Convolutional   imagenet   \n",
       "195        visformer_small          Visformer  Convolutional   imagenet   \n",
       "95   vit_small_patch32_224          ViT-S-P32    Transformer   imagenet   \n",
       "\n",
       "    train_type train_data                                        description  \\\n",
       "138     random       None  HaloNet-26T randomly initialized, with no trai...   \n",
       "195     random       None  Visformer randomly initialized, with no training.   \n",
       "95    imagenet   imagenet  ViT-S-P32 trained on image classification with...   \n",
       "\n",
       "    task_cluster model_source  \\\n",
       "138     Semantic         timm   \n",
       "195     Semantic         timm   \n",
       "95      Semantic         timm   \n",
       "\n",
       "                                      model_source_url  \n",
       "138  https://github.com/rwightman/pytorch-image-mod...  \n",
       "195  https://github.com/rwightman/pytorch-image-mod...  \n",
       "95   https://github.com/rwightman/pytorch-image-mod...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clip Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_dictlist = [\n",
    "    \n",
    "    {'model': 'RN50', 'model_display_name': 'CLiP-ResNet50', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'RN101', 'model_display_name': 'CLiP-ResNet101', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'RN50x4', 'model_display_name': 'CLiP-ResNet50x4', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'RN50x16', 'model_display_name': 'CLiP-ResNet50x16', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'ViT-B/32', 'model_display_name': 'CLiP-ViT-B/32', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'ViT-B/16', 'model_display_name': 'CLiP-ViT-B/32', 'model_class': 'Transformer'},\n",
    "    \n",
    "]\n",
    "    \n",
    "    \n",
    "for i, dict_i in enumerate(clip_dictlist):\n",
    "    dict_i = clip_dictlist[i]\n",
    "    dict_i['description'] = '{}, a hybrid vision-language model.'.format(dict_i['model_display_name'])\n",
    "    \n",
    "clip_df = pd.DataFrame(clip_dictlist)\n",
    "clip_df['model_type'] = 'clip'\n",
    "clip_df['train_type'] = 'clip'\n",
    "clip_df['train_data'] = 'internet'\n",
    "clip_df['task_cluster'] = 'Vision-Language'\n",
    "clip_df['model_source'] = 'clip'\n",
    "clip_df['model_source_url'] = 'https://github.com/openai/CLIP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>model_class</th>\n",
       "      <th>description</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RN101</td>\n",
       "      <td>CLiP-ResNet101</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>CLiP-ResNet101, a hybrid vision-language model.</td>\n",
       "      <td>clip</td>\n",
       "      <td>clip</td>\n",
       "      <td>internet</td>\n",
       "      <td>Vision-Language</td>\n",
       "      <td>clip</td>\n",
       "      <td>https://github.com/openai/CLIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RN50x4</td>\n",
       "      <td>CLiP-ResNet50x4</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>CLiP-ResNet50x4, a hybrid vision-language model.</td>\n",
       "      <td>clip</td>\n",
       "      <td>clip</td>\n",
       "      <td>internet</td>\n",
       "      <td>Vision-Language</td>\n",
       "      <td>clip</td>\n",
       "      <td>https://github.com/openai/CLIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RN50x16</td>\n",
       "      <td>CLiP-ResNet50x16</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>CLiP-ResNet50x16, a hybrid vision-language model.</td>\n",
       "      <td>clip</td>\n",
       "      <td>clip</td>\n",
       "      <td>internet</td>\n",
       "      <td>Vision-Language</td>\n",
       "      <td>clip</td>\n",
       "      <td>https://github.com/openai/CLIP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model model_display_name    model_class  \\\n",
       "1    RN101     CLiP-ResNet101  Convolutional   \n",
       "2   RN50x4    CLiP-ResNet50x4  Convolutional   \n",
       "3  RN50x16   CLiP-ResNet50x16  Convolutional   \n",
       "\n",
       "                                         description model_type train_type  \\\n",
       "1    CLiP-ResNet101, a hybrid vision-language model.       clip       clip   \n",
       "2   CLiP-ResNet50x4, a hybrid vision-language model.       clip       clip   \n",
       "3  CLiP-ResNet50x16, a hybrid vision-language model.       clip       clip   \n",
       "\n",
       "  train_data     task_cluster model_source                model_source_url  \n",
       "1   internet  Vision-Language         clip  https://github.com/openai/CLIP  \n",
       "2   internet  Vision-Language         clip  https://github.com/openai/CLIP  \n",
       "3   internet  Vision-Language         clip  https://github.com/openai/CLIP  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISSL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vissl_dictlist = [\n",
    "    {'model': 'ResNet50-JigSaw-P100',\n",
    "  'weights_url': ' https://dl.fbaipublicfiles.com/vissl/model_zoo/jigsaw_rn50_in1k_ep105_perm2k_jigsaw_8gpu_resnet_17_07_20.db174a43/model_final_checkpoint_phase104.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-JigSaw-Goyal19',\n",
    "  'weights_url': ' https://dl.fbaipublicfiles.com/vissl/model_zoo/converted_vissl_rn50_jigsaw_in1k_goyal19.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-RotNet',\n",
    "  'weights_url': ' https://dl.fbaipublicfiles.com/vissl/model_zoo/rotnet_rn50_in1k_ep105_rotnet_8gpu_resnet_17_07_20.46bada9f/model_final_checkpoint_phase125.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-ClusterFit-16K-RotNet',\n",
    "  'weights_url': ' https://dl.fbaipublicfiles.com/vissl/model_zoo/converted_vissl_rn50_rotnet_16kclusters_in1k_ep105.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-NPID-4KNegative',\n",
    "  'weights_url': ' https://dl.fbaipublicfiles.com/vissl/model_zoo/npid_1node_200ep_4kneg_npid_8gpu_resnet_23_07_20.9eb36512/model_final_checkpoint_phase199.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-PIRL',\n",
    "  'weights_url': ' https://dl.fbaipublicfiles.com/vissl/model_zoo/pirl_jigsaw_4node_pirl_jigsaw_4node_resnet_22_07_20.34377f59/model_final_checkpoint_phase799.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-SimCLR',\n",
    "  'weights_url': ' https://dl.fbaipublicfiles.com/vissl/model_zoo/simclr_rn50_1000ep_simclr_8node_resnet_16_07_20.afe428c7/model_final_checkpoint_phase999.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-DeepClusterV2-2x224',\n",
    "  'weights_url': ' https://dl.fbaipublicfiles.com/vissl/model_zoo/deepclusterv2_400ep_2x224_pretrain.pth.tar'},\n",
    " \n",
    "    {'model': 'ResNet50-DeepClusterV2-2x224+6x96',\n",
    "  'weights_url': ' https://dl.fbaipublicfiles.com/vissl/model_zoo/deepclusterv2_800ep_pretrain.pth.tar'},\n",
    " \n",
    "    {'model': 'ResNet50-SwAV-BS4096-2x224',\n",
    "  'weights_url': ' https://dl.fbaipublicfiles.com/vissl/model_zoo/swav_in1k_rn50_400ep_swav_8node_resnet_27_07_20.a5990fc9/model_final_checkpoint_phase399.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-SwAV-BS4096-2x224+6x96',\n",
    "  'weights_url': ' https://dl.fbaipublicfiles.com/vissl/model_zoo/swav_8node_2x224_rn50_in1k_swav_8node_resnet_30_07_20.c8fd7169/model_final_checkpoint_phase399.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-MoCoV2-BS256',\n",
    "  'weights_url': ' https://dl.fbaipublicfiles.com/vissl/model_zoo/moco_v2_1node_lr.03_step_b32_zero_init/model_final_checkpoint_phase199.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-BarlowTwins-BS2048',\n",
    "  'weights_url': ' https://dl.fbaipublicfiles.com/vissl/model_zoo/barlow_twins/barlow_twins_32gpus_4node_imagenet1k_1000ep_resnet50.torch'}\n",
    "]\n",
    "\n",
    "for i, dict_i in enumerate(vissl_dictlist):\n",
    "    dict_i = vissl_dictlist[i]\n",
    "    dict_i['model_display_name'] = dict_i['model']\n",
    "    dict_i['description'] = '{}, a self-supervised representation learner trained on ImageNet.'.format(dict_i['model'])\n",
    "    \n",
    "vissl_df = pd.DataFrame(vissl_dictlist)\n",
    "vissl_df['model_type'] = 'selfsupervised'\n",
    "vissl_df['train_type'] = 'selfsupervised'\n",
    "vissl_df['train_data'] = 'imagenet'\n",
    "vissl_df['task_cluster'] = 'Self-Supervised'\n",
    "vissl_df['model_source'] = 'vissl'\n",
    "vissl_df['model_class'] = 'Convolutional'\n",
    "vissl_df['model_source_url'] = 'https://github.com/facebookresearch/vissl/'\n",
    "\n",
    "vissl_df['model_display_name'] = vissl_df['model_display_name'].apply(lambda x: x.replace('-2x224', ''))\n",
    "vissl_df['model_display_name'] = vissl_df['model_display_name'].apply(lambda x: x.replace('+6x96', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>weights_url</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>description</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ResNet50-SwAV-BS4096-2x224</td>\n",
       "      <td>https://dl.fbaipublicfiles.com/vissl/model_zo...</td>\n",
       "      <td>ResNet50-SwAV-BS4096</td>\n",
       "      <td>ResNet50-SwAV-BS4096-2x224, a self-supervised ...</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Self-Supervised</td>\n",
       "      <td>vissl</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ResNet50-ClusterFit-16K-RotNet</td>\n",
       "      <td>https://dl.fbaipublicfiles.com/vissl/model_zo...</td>\n",
       "      <td>ResNet50-ClusterFit-16K-RotNet</td>\n",
       "      <td>ResNet50-ClusterFit-16K-RotNet, a self-supervi...</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Self-Supervised</td>\n",
       "      <td>vissl</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ResNet50-PIRL</td>\n",
       "      <td>https://dl.fbaipublicfiles.com/vissl/model_zo...</td>\n",
       "      <td>ResNet50-PIRL</td>\n",
       "      <td>ResNet50-PIRL, a self-supervised representatio...</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Self-Supervised</td>\n",
       "      <td>vissl</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model  \\\n",
       "9      ResNet50-SwAV-BS4096-2x224   \n",
       "3  ResNet50-ClusterFit-16K-RotNet   \n",
       "5                   ResNet50-PIRL   \n",
       "\n",
       "                                         weights_url  \\\n",
       "9   https://dl.fbaipublicfiles.com/vissl/model_zo...   \n",
       "3   https://dl.fbaipublicfiles.com/vissl/model_zo...   \n",
       "5   https://dl.fbaipublicfiles.com/vissl/model_zo...   \n",
       "\n",
       "               model_display_name  \\\n",
       "9            ResNet50-SwAV-BS4096   \n",
       "3  ResNet50-ClusterFit-16K-RotNet   \n",
       "5                   ResNet50-PIRL   \n",
       "\n",
       "                                         description      model_type  \\\n",
       "9  ResNet50-SwAV-BS4096-2x224, a self-supervised ...  selfsupervised   \n",
       "3  ResNet50-ClusterFit-16K-RotNet, a self-supervi...  selfsupervised   \n",
       "5  ResNet50-PIRL, a self-supervised representatio...  selfsupervised   \n",
       "\n",
       "       train_type train_data     task_cluster model_source    model_class  \\\n",
       "9  selfsupervised   imagenet  Self-Supervised        vissl  Convolutional   \n",
       "3  selfsupervised   imagenet  Self-Supervised        vissl  Convolutional   \n",
       "5  selfsupervised   imagenet  Self-Supervised        vissl  Convolutional   \n",
       "\n",
       "                             model_source_url  \n",
       "9  https://github.com/facebookresearch/vissl/  \n",
       "3  https://github.com/facebookresearch/vissl/  \n",
       "5  https://github.com/facebookresearch/vissl/  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vissl_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dino Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_models = ['dino_vits16',\n",
    "               'dino_vits8',\n",
    "               'dino_vitb16',\n",
    "               'dino_vitb8',\n",
    "               'dino_xcit_small_12_p16',\n",
    "               'dino_xcit_small_12_p8',\n",
    "               'dino_xcit_medium_24_p16',\n",
    "               'dino_xcit_medium_24_p8',\n",
    "               'dino_resnet50']\n",
    "\n",
    "\n",
    "dino_dictlist = [\n",
    "    \n",
    "    {'model': 'dino_vits16', 'model_display_name': 'Dino-VIT-S16'},\n",
    "    \n",
    "    {'model': 'dino_vits8', 'model_display_name': 'Dino-VIT-S8'},\n",
    "    \n",
    "    {'model': 'dino_vitb16', 'model_display_name': 'Dino-VIT-B16'},\n",
    "    \n",
    "    {'model': 'dino_vitb8', 'model_display_name': 'Dino-VIT-B8'},\n",
    "    \n",
    "    {'model': 'dino_xcit_small_12_p16', 'model_display_name': 'Dino-XCIT-S12-P16'},\n",
    "    \n",
    "    {'model': 'dino_xcit_small_12_p8', 'model_display_name': 'Dino-XCIT-S12-P8'},\n",
    "    \n",
    "    {'model': 'dino_xcit_medium_24_p16', 'model_display_name': 'Dino-XCIT-M24-P16'},\n",
    "    \n",
    "    {'model': 'dino_xcit_medium_24_p8', 'model_display_name': 'Dino-XCIT-M24-P8'},\n",
    "    \n",
    "    {'model': 'dino_resnet50', 'model_display_name': 'Dino-ResNet50'}\n",
    "    \n",
    "]\n",
    "\n",
    "train_type_text = {\n",
    "    'selfsupervised': 'self supervision',\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "}   \n",
    "    \n",
    "for i, dict_i in enumerate(dino_dictlist):\n",
    "    dict_i = dino_dictlist[i]\n",
    "    dict_i['model_type'] = 'selfsupervised'\n",
    "    dict_i['train_type'] = 'selfsupervised'\n",
    "    model_class = 'Transformer'\n",
    "    if 'resnet50' in dict_i['model']:\n",
    "        model_class = 'Convolutional'\n",
    "    dict_i['model_class'] = model_class\n",
    "    dict_i['train_data'] = 'imagenet'\n",
    "    dict_i['description'] = '{} trained via {} with the {} dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                           train_type_text[dict_i['train_type']],\n",
    "                                                                           train_data_text[dict_i['train_data']])\n",
    "    \n",
    "dino_df = pd.DataFrame(dino_dictlist)\n",
    "dino_df['task_cluster'] = 'SelfSupervised'\n",
    "dino_df['model_source'] = 'dino'\n",
    "dino_df['model_source_url'] = 'https://github.com/facebookresearch/dino'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_type</th>\n",
       "      <th>model_class</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dino_resnet50</td>\n",
       "      <td>Dino-ResNet50</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Dino-ResNet50 trained via self supervision wit...</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>dino</td>\n",
       "      <td>https://github.com/facebookresearch/dino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dino_vitb8</td>\n",
       "      <td>Dino-VIT-B8</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Dino-VIT-B8 trained via self supervision with ...</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>dino</td>\n",
       "      <td>https://github.com/facebookresearch/dino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dino_vits8</td>\n",
       "      <td>Dino-VIT-S8</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Dino-VIT-S8 trained via self supervision with ...</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>dino</td>\n",
       "      <td>https://github.com/facebookresearch/dino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model model_display_name      model_type      train_type  \\\n",
       "8  dino_resnet50      Dino-ResNet50  selfsupervised  selfsupervised   \n",
       "3     dino_vitb8        Dino-VIT-B8  selfsupervised  selfsupervised   \n",
       "1     dino_vits8        Dino-VIT-S8  selfsupervised  selfsupervised   \n",
       "\n",
       "     model_class train_data  \\\n",
       "8  Convolutional   imagenet   \n",
       "3    Transformer   imagenet   \n",
       "1    Transformer   imagenet   \n",
       "\n",
       "                                         description    task_cluster  \\\n",
       "8  Dino-ResNet50 trained via self supervision wit...  SelfSupervised   \n",
       "3  Dino-VIT-B8 trained via self supervision with ...  SelfSupervised   \n",
       "1  Dino-VIT-S8 trained via self supervision with ...  SelfSupervised   \n",
       "\n",
       "  model_source                          model_source_url  \n",
       "8         dino  https://github.com/facebookresearch/dino  \n",
       "3         dino  https://github.com/facebookresearch/dino  \n",
       "1         dino  https://github.com/facebookresearch/dino  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiDas Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "midas_dictlist = [\n",
    "    {'model': 'DPT_Hybrid', 'model_display_name': 'DPT-Hybrid'},\n",
    "    {'model': 'DPT_Large', 'model_display_name': 'DPT-Large'},\n",
    "    {'model': 'MiDas', 'model_display_name': 'MiDas'},\n",
    "    {'model': 'MiDas_small', 'model_display_name': 'MiDas-Small'} \n",
    "]\n",
    "\n",
    "for i, dict_i in enumerate(midas_dictlist):\n",
    "    dict_i = midas_dictlist[i]\n",
    "    dict_i['description'] = '{}, a monocular depth estimation model.'.format(dict_i['model_display_name'])\n",
    "    \n",
    "midas_df = pd.DataFrame(midas_dictlist)\n",
    "midas_df['model_type'] = 'monoculardepth'\n",
    "midas_df['train_type'] = 'monoculardepth'\n",
    "midas_df['train_data'] = 'multiple'\n",
    "midas_df['task_cluster'] = 'MonocularDepth'\n",
    "midas_df['model_source'] = 'midas'\n",
    "midas_df['model_source_url'] = 'https://github.com/isl-org/MiDaS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>description</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MiDas_small</td>\n",
       "      <td>MiDas-Small</td>\n",
       "      <td>MiDas-Small, a monocular depth estimation model.</td>\n",
       "      <td>monoculardepth</td>\n",
       "      <td>monoculardepth</td>\n",
       "      <td>multiple</td>\n",
       "      <td>MonocularDepth</td>\n",
       "      <td>midas</td>\n",
       "      <td>https://github.com/isl-org/MiDaS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DPT_Large</td>\n",
       "      <td>DPT-Large</td>\n",
       "      <td>DPT-Large, a monocular depth estimation model.</td>\n",
       "      <td>monoculardepth</td>\n",
       "      <td>monoculardepth</td>\n",
       "      <td>multiple</td>\n",
       "      <td>MonocularDepth</td>\n",
       "      <td>midas</td>\n",
       "      <td>https://github.com/isl-org/MiDaS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MiDas</td>\n",
       "      <td>MiDas</td>\n",
       "      <td>MiDas, a monocular depth estimation model.</td>\n",
       "      <td>monoculardepth</td>\n",
       "      <td>monoculardepth</td>\n",
       "      <td>multiple</td>\n",
       "      <td>MonocularDepth</td>\n",
       "      <td>midas</td>\n",
       "      <td>https://github.com/isl-org/MiDaS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model model_display_name  \\\n",
       "3  MiDas_small        MiDas-Small   \n",
       "1    DPT_Large          DPT-Large   \n",
       "2        MiDas              MiDas   \n",
       "\n",
       "                                        description      model_type  \\\n",
       "3  MiDas-Small, a monocular depth estimation model.  monoculardepth   \n",
       "1    DPT-Large, a monocular depth estimation model.  monoculardepth   \n",
       "2        MiDas, a monocular depth estimation model.  monoculardepth   \n",
       "\n",
       "       train_type train_data    task_cluster model_source  \\\n",
       "3  monoculardepth   multiple  MonocularDepth        midas   \n",
       "1  monoculardepth   multiple  MonocularDepth        midas   \n",
       "2  monoculardepth   multiple  MonocularDepth        midas   \n",
       "\n",
       "                   model_source_url  \n",
       "3  https://github.com/isl-org/MiDaS  \n",
       "1  https://github.com/isl-org/MiDaS  \n",
       "2  https://github.com/isl-org/MiDaS  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midas_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detectron Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectron_dictlist = [\n",
    "    {'model': 'faster_rcnn_R_50_C4_3x', 'model_display_name': 'Faster-RCNN-ResNet50-C4',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_50_C4_3x.yaml'},\n",
    "        \n",
    "    {'model': 'faster_rcnn_R_50_DC5_3x', 'model_display_name': 'Faster-RCNN-ResNet50-DC5',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_50_DC5_3x.yaml'},\n",
    "    \n",
    "    {'model': 'faster_rcnn_R_50_FPN_3x', 'model_display_name': 'Faster-RCNN-ResNet50-FPN',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'faster_rcnn_R_101_C4_3x', 'model_display_name': 'Faster-RCNN-ResNet101-C4',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_101_C4_3x.yaml'},\n",
    "        \n",
    "    {'model': 'faster_rcnn_R_101_DC5_3x', 'model_display_name': 'Faster-RCNN-ResNet101-DC5',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_101_DC5_3x.yaml'},\n",
    "    \n",
    "    {'model': 'faster_rcnn_R_101_FPN_3x', 'model_display_name': 'Faster-RCNN-ResNet101-FPN',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'faster_rcnn_X_101_32x8d_FPN_3x', 'model_display_name': 'Faster-RCNN-X101-FPN',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'retinanet_R_50_FPN_3x', 'model_display_name': 'RetinaNet-ResNet50-FPN',\n",
    "     'weights_url': 'COCO-Detection/retinanet_R_50_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'retinanet_R_101_FPN_3x', 'model_display_name': 'RetinaNet-ResNet101-FPN',\n",
    "     'weights_url': 'COCO-Detection/retinanet_R_101_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_50_C4_3x', 'model_display_name': 'Mask-RCNN-ResNet50-C4',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_50_DC5_3x', 'model_display_name': 'Mask-RCNN-ResNet50-DC5',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_50_FPN_3x', 'model_display_name': 'Mask-RCNN-ResNet50-FPN',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_101_C4_3x', 'model_display_name': 'Mask-RCNN-ResNet101-C4',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_101_DC5_3x', 'model_display_name': 'Mask-RCNN-ResNet101-DC5',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_101_DC5_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_101_FPN_3x', 'model_display_name': 'Mask-RCNN-ResNet101-FPN',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_X_101_32x8d_FPN_3x', 'model_display_name': 'Mask-RCNN-X101-FPN',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml'},\n",
    "        \n",
    "    {'model': 'keypoint_rcnn_R_50_FPN_3x', 'model_display_name': 'Keypoint-RCNN-ResNet50-FPN',\n",
    "     'weights_url': 'COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'panoptic_fpn_R_50_3x', 'model_display_name': 'Panoptic-RCNN-ResNet50-FPN',\n",
    "     'weights_url': 'COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_50_FPN_1x', 'model_display_name': 'LVIS-RCNN-ResNet50-FPN',\n",
    "     'weights_url': 'LVISv0.5-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_101_FPN_1x', 'model_display_name': 'LVIS-RCNN-ResNet101-FPN',\n",
    "     'weights_url': 'LVISv0.5-InstanceSegmentation/mask_rcnn_R_101_FPN_1x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_X_101_32x8d_FPN_1x', 'model_display_name': 'LVIS-RCNN-X101-FPN',\n",
    "     'weights_url': 'LVISv0.5-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x.yaml'},\n",
    "    \n",
    "]\n",
    "\n",
    "for i, dict_i in enumerate(detectron_dictlist):\n",
    "    dict_i = detectron_dictlist[i]\n",
    "    train_data = 'coco2017'\n",
    "    if 'COCO-Detection' in dict_i['weights_url']:\n",
    "        train_type = 'detection'\n",
    "        model_type = 'detection'\n",
    "    if 'COCO-InstanceSegmentation' in dict_i['weights_url']:\n",
    "        train_type = 'segmentation'\n",
    "        model_type = 'segmentation'\n",
    "    if 'new_baselines' in dict_i['weights_url']:\n",
    "        train_type = 'segmentation'\n",
    "        model_type = 'segmentation'\n",
    "    if 'CoCo-Keypoints' in dict_i['weights_url']:\n",
    "        train_type = 'keypoints'\n",
    "        model_type = 'keypoints'\n",
    "    if 'COCO-PanopticSegmentation' in dict_i['weights_url']:\n",
    "        train_type = 'panoptics'\n",
    "        model_type = 'panoptics'\n",
    "    if 'COCO-PanopticSegmentation' in dict_i['weights_url']:\n",
    "        train_type = 'panoptics'\n",
    "        model_type = 'panoptics'\n",
    "    if 'LVISv0.5-InstanceSegmentation' in dict_i['weights_url']:\n",
    "        train_type = 'segmentation'\n",
    "        model_type = 'segmentation'\n",
    "        train_data = 'LVIS'\n",
    "    dict_i['model_type'] = model_type\n",
    "    dict_i['train_type'] = train_type\n",
    "    dict_i['train_data'] = train_data\n",
    "    dict_i['description'] = '{}, trained on {} with the CoCo2017 dataset.'.format(dict_i['model_display_name'], train_type)\n",
    "    \n",
    "detectron_df = pd.DataFrame(detectron_dictlist)\n",
    "detectron_df['task_cluster'] = 'Detection|Segmentation'\n",
    "detectron_df['model_source'] = 'detectron'\n",
    "detectron_df['model_source_url'] = 'https://github.com/facebookresearch/detectron2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>weights_url</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mask_rcnn_X_101_32x8d_FPN_1x</td>\n",
       "      <td>LVIS-RCNN-X101-FPN</td>\n",
       "      <td>LVISv0.5-InstanceSegmentation/mask_rcnn_X_101_...</td>\n",
       "      <td>segmentation</td>\n",
       "      <td>segmentation</td>\n",
       "      <td>LVIS</td>\n",
       "      <td>LVIS-RCNN-X101-FPN, trained on segmentation wi...</td>\n",
       "      <td>Detection|Segmentation</td>\n",
       "      <td>detectron</td>\n",
       "      <td>https://github.com/facebookresearch/detectron2/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>faster_rcnn_R_101_C4_3x</td>\n",
       "      <td>Faster-RCNN-ResNet101-C4</td>\n",
       "      <td>COCO-Detection/faster_rcnn_R_101_C4_3x.yaml</td>\n",
       "      <td>detection</td>\n",
       "      <td>detection</td>\n",
       "      <td>coco2017</td>\n",
       "      <td>Faster-RCNN-ResNet101-C4, trained on detection...</td>\n",
       "      <td>Detection|Segmentation</td>\n",
       "      <td>detectron</td>\n",
       "      <td>https://github.com/facebookresearch/detectron2/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>keypoint_rcnn_R_50_FPN_3x</td>\n",
       "      <td>Keypoint-RCNN-ResNet50-FPN</td>\n",
       "      <td>COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml</td>\n",
       "      <td>segmentation</td>\n",
       "      <td>segmentation</td>\n",
       "      <td>coco2017</td>\n",
       "      <td>Keypoint-RCNN-ResNet50-FPN, trained on segment...</td>\n",
       "      <td>Detection|Segmentation</td>\n",
       "      <td>detectron</td>\n",
       "      <td>https://github.com/facebookresearch/detectron2/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model          model_display_name  \\\n",
       "20  mask_rcnn_X_101_32x8d_FPN_1x          LVIS-RCNN-X101-FPN   \n",
       "3        faster_rcnn_R_101_C4_3x    Faster-RCNN-ResNet101-C4   \n",
       "16     keypoint_rcnn_R_50_FPN_3x  Keypoint-RCNN-ResNet50-FPN   \n",
       "\n",
       "                                          weights_url    model_type  \\\n",
       "20  LVISv0.5-InstanceSegmentation/mask_rcnn_X_101_...  segmentation   \n",
       "3         COCO-Detection/faster_rcnn_R_101_C4_3x.yaml     detection   \n",
       "16      COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml  segmentation   \n",
       "\n",
       "      train_type train_data  \\\n",
       "20  segmentation       LVIS   \n",
       "3      detection   coco2017   \n",
       "16  segmentation   coco2017   \n",
       "\n",
       "                                          description            task_cluster  \\\n",
       "20  LVIS-RCNN-X101-FPN, trained on segmentation wi...  Detection|Segmentation   \n",
       "3   Faster-RCNN-ResNet101-C4, trained on detection...  Detection|Segmentation   \n",
       "16  Keypoint-RCNN-ResNet50-FPN, trained on segment...  Detection|Segmentation   \n",
       "\n",
       "   model_source                                 model_source_url  \n",
       "20    detectron  https://github.com/facebookresearch/detectron2/  \n",
       "3     detectron  https://github.com/facebookresearch/detectron2/  \n",
       "16    detectron  https://github.com/facebookresearch/detectron2/  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detectron_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [torchvision_df, taskonomy_df, timm_df, clip_df, vissl_df, dino_df, midas_df, detectron_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "      <th>weights_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>wide_resnet50_2</td>\n",
       "      <td>Wide-ResNet50</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>random</td>\n",
       "      <td>None</td>\n",
       "      <td>Wide-ResNet50 randomly initialized, with no tr...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>efficientnet_b1</td>\n",
       "      <td>EfficientNet-B1</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>random</td>\n",
       "      <td>None</td>\n",
       "      <td>EfficientNet-B1 randomly initialized, with no ...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faster_rcnn_R_50_C4_3x</td>\n",
       "      <td>Faster-RCNN-ResNet50-C4</td>\n",
       "      <td>detection</td>\n",
       "      <td>detection</td>\n",
       "      <td>coco2017</td>\n",
       "      <td>Faster-RCNN-ResNet50-C4, trained on detection ...</td>\n",
       "      <td>Detection|Segmentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>detectron</td>\n",
       "      <td>https://github.com/facebookresearch/detectron2/</td>\n",
       "      <td>COCO-Detection/faster_rcnn_R_50_C4_3x.yaml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model       model_display_name model_type train_type  \\\n",
       "56          wide_resnet50_2            Wide-ResNet50   imagenet     random   \n",
       "128         efficientnet_b1          EfficientNet-B1   imagenet     random   \n",
       "0    faster_rcnn_R_50_C4_3x  Faster-RCNN-ResNet50-C4  detection  detection   \n",
       "\n",
       "    train_data                                        description  \\\n",
       "56        None  Wide-ResNet50 randomly initialized, with no tr...   \n",
       "128       None  EfficientNet-B1 randomly initialized, with no ...   \n",
       "0     coco2017  Faster-RCNN-ResNet50-C4, trained on detection ...   \n",
       "\n",
       "               task_cluster    model_class model_source  \\\n",
       "56                 Semantic  Convolutional  torchvision   \n",
       "128                Semantic  Convolutional         timm   \n",
       "0    Detection|Segmentation            NaN    detectron   \n",
       "\n",
       "                                      model_source_url  \\\n",
       "56     pytorch.org/docs/stable/torchvision/models.html   \n",
       "128  https://github.com/rwightman/pytorch-image-mod...   \n",
       "0      https://github.com/facebookresearch/detectron2/   \n",
       "\n",
       "                                    weights_url  \n",
       "56                                          NaN  \n",
       "128                                         NaN  \n",
       "0    COCO-Detection/faster_rcnn_R_50_C4_3x.yaml  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(df_list).sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(df_list).to_csv('model_typology.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the Lab Slab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-ead8bdc22d19>, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-ead8bdc22d19>\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    'semi-supervised': 'semi-supervision',\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "timm_alternative_dictlist = [\n",
    "\n",
    "    {'model': 'mixer_b16_224_miil_in21k', 'model_display_name': 'Mixer-B16-IN22K', 'model_class': 'MLP-Mixer',\n",
    "     'model_type': 'imagenet21k', 'train_type': 'imagenet21k', 'train_data': 'imagenet21k'},\n",
    "    \n",
    "    {'model': 'mobilenetv3_large_100_miil_in21k', 'model_display_name': 'MobileNet-V3-Large-IN22K', \n",
    "     'model_class': 'Convolutional', \n",
    "     'model_type': 'imagenet21k', 'train_type': 'imagenet21k', 'train_data': 'imagenet21k'},\n",
    "    \n",
    "    {'model': 'resnetv2_50x1_bitm_in21k', 'model_display_name': 'ResNetV2-50x1-BitM-IN22K', 'model_class': 'MLP-Mixer',\n",
    "     'model_type': 'imagenet21k', 'train_type': 'imagenet21k', 'train_data': 'imagenet21k'},\n",
    "    \n",
    "    {'model': 'vit_base_patch16_224_miil_in21k', 'model_display_name': 'ViT-B-P16-IN22K', 'model_class': 'MLP-Mixer',\n",
    "     'model_type': 'imagenet21k', 'train_type': 'imagenet21k', 'train_data': 'imagenet21k'},\n",
    "    \n",
    "    {'model': 'ssl_resnet18', 'model_display_name': 'ResNet18-SSL', 'model_class': 'Convolutional',\n",
    "     'model_type': 'semi-supervised', 'train_type': 'semi-supervised', 'train_data': 'YFCC100M'},\n",
    "    \n",
    "    {'model': 'ssl_resnet50', 'model_display_name': 'ResNet50-SSL', 'model_class': 'Convolutional',\n",
    "     'model_type': 'semi-supervised', 'train_type': 'semi-supervised', 'train_data': 'YFCC100M'},\n",
    "    \n",
    "    {'model': 'ssl_resnext101_32x4d', 'model_display_name': 'ResNext101-32x4D-SSL', 'model_class': 'Convolutional',\n",
    "     'model_type': 'semi-supervised', 'train_type': 'semi-supervised', 'train_data': 'YFCC100M'},\n",
    "    \n",
    "    {'model': 'swsl_resnet18', 'model_display_name': 'Resnet18-SWSL', 'model_class': 'Convolutional',\n",
    "     'model_type': 'semi-weakly-supervised', 'train_type': 'semi-weakly-supervised', 'train_data': 'Instagram'},\n",
    "    \n",
    "    {'model': 'swsl_resnet50', 'model_display_name': 'Resnet50-SWSL', 'model_class': 'Convolutional',\n",
    "     'model_type': 'semi-weakly-supervised', 'train_type': 'semi-weakly-supervised', 'train_data': 'Instagram'},\n",
    "    \n",
    "    {'model': 'swsl_resnext101_32x4d', 'model_display_name': 'ResNext101-32x4D-SWSL', 'model_class': 'Convolutional',\n",
    "     'model_type': 'semi-weakly-supervised', 'train_type': 'semi-weakly-supervised', 'train_data': 'Instagram'},\n",
    "    \n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "train_type_text = {\n",
    "    'imagenet': 'image classification',\n",
    "    'imagenet21k': '21000-way image classification',\n",
    "    'semi-supervised': 'semi-supervision',\n",
    "    'semi-weakly-supervised': 'semi-weak-supervision',\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "    'imagenet21k': 'ImageNet21K'\n",
    "    'YFCC100M': 'YFCC100M',\n",
    "    'Instagram': 'Instagram', \n",
    "}\n",
    "\n",
    "for i, dict_i in enumerate(timm_dictlist):\n",
    "    dict_i = timm_dictlist[i]\n",
    "    if dict_i['train_type'] != 'random':\n",
    "        dict_i['description'] = '{} trained on {} with the {} dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                               train_type_text[dict_i['train_type']],\n",
    "                                                                               train_data_text[dict_i['train_data']])\n",
    "    if dict_i['train_type'] == 'random':\n",
    "        dict_i['description'] = '{} randomly initialized, with no training.'.format(dict_i['model_display_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision_alternative_dictlist = [\n",
    "    \n",
    "    {'model': 'deeplabv3_resnet101', 'model_display_name': 'DeepLabV3-Resnet101',\n",
    "     'model_type': 'segmentation', 'train_type': 'segmentation', 'train_data': 'coco2017'},\n",
    "    \n",
    "    {'model': 'fcn_resnet101', 'model_display_name': 'FCN-Resnet101', 'model_type': 'detection', \n",
    "     'model_type': 'segmentation', 'train_type': 'segmentation', 'train_data': 'coco2017'},\n",
    "    \n",
    "    {'model': 'fasterrcnn_resnet50_fpn', 'model_display_name': 'FasterRCNN-Resnet50-FPN', \n",
    "     'model_type': 'detection', 'train_type': 'detection', 'train_data': 'coco2017'},\n",
    "    \n",
    "    {'model': 'maskrcnn_resnet50_fpn', 'model_display_name': 'MaskRCNN-Resnet50-FPN', \n",
    "     'model_type': 'detection', 'train_type': 'detection', 'train_data': 'coco2017'},\n",
    "    \n",
    "    {'model': 'keypointrcnn_resnet50_fpn', 'model_display_name': 'KeyPointRCNN-Resnet50-FPN', \n",
    "     'model_type': 'detection', 'train_type': 'detection', 'train_data': 'coco2017'},\n",
    "    \n",
    "    {'model': 'r3d_18', 'model_display_name': 'R3D-18', 'model_type': 'video', \n",
    "     'train_type': 'video', 'train_data': 'kinetics400'},\n",
    "    \n",
    "    {'model': 'r2plus1d_18', 'model_display_name': 'R2Plus1D-18', 'model_type': 'video', \n",
    "     'train_type': 'video', 'train_data': 'kinetics400'},\n",
    "    \n",
    "    {'model': 'mc3_18', 'model_display_name': 'MC3-18','model_type': 'video', \n",
    "     'train_type': 'video', 'train_data': 'kinetics400'}\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
