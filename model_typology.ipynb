{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision_dictlist = [\n",
    "    \n",
    "    {'model': 'alexnet', 'model_display_name': 'AlexNet', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'vgg11', 'model_display_name': 'VGG11', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg13', 'model_display_name': 'VGG13', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg16', 'model_display_name': 'VGG16', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg19', 'model_display_name': 'VGG19', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg11_bn', 'model_display_name': 'VGG11-BatchNorm', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg13_bn', 'model_display_name': 'VGG13-BatchNorm', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg16_bn', 'model_display_name': 'VGG16-BatchNorm', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg19_bn', 'model_display_name': 'VGG19-BatchNorm', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'resnet18', 'model_display_name': 'ResNet18', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'resnet34', 'model_display_name': 'ResNet34', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnet50', 'model_display_name': 'ResNet50', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnet101', 'model_display_name': 'ResNet101', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnet152', 'model_display_name': 'ResNet152', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'squeezenet1_0', 'model_display_name': 'SqueezeNet1.0', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'squeezenet1_1', 'model_display_name': 'SqueezeNet1.1', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet121', 'model_display_name': 'DenseNet121', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet161', 'model_display_name': 'DenseNet161', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet169', 'model_display_name': 'DenseNet169', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet201', 'model_display_name': 'DenseNet201', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'googlenet', 'model_display_name': 'GoogleNet', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'shufflenet_v2_x0_5', 'model_display_name': 'ShuffleNet-V2-x0.5', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    " \n",
    "    {'model': 'shufflenet_v2_x1_0', 'model_display_name': 'ShuffleNet-V2-x1.0', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mobilenet_v2', 'model_display_name': 'MobileNet-V2', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnext50_32x4d', 'model_display_name': 'ResNext50-32x4D', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnext101_32x8d', 'model_display_name': 'ResNext50-32x8D', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'wide_resnet50_2', 'model_display_name': 'Wide-ResNet50', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'wide_resnet101_2', 'model_display_name': 'Wide-ResNet101', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mnasnet0_5', 'model_display_name': 'MNASNet0.5', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mnasnet1_0', 'model_display_name': 'MNASNet1.0', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'inception_v3', 'model_display_name': 'Inception-V3', \n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'fasterrcnn_resnet50_fpn', 'model_display_name': 'FasterRCNN-Resnet50-FPN', \n",
    "     'model_type': 'segmentation', 'train_type': 'segmentation', 'train_data': 'coco2017'},\n",
    "    \n",
    "    {'model': 'maskrcnn_resnet50_fpn', 'model_display_name': 'MaskRCNN-Resnet50-FPN', \n",
    "     'model_type': 'segmentation', 'train_type': 'segmentation', 'train_data': 'coco2017'},\n",
    "    \n",
    "    {'model': 'keypointrcnn_resnet50_fpn', 'model_display_name': 'KeyPointRCNN-Resnet50-FPN', \n",
    "     'model_type': 'segmentation', 'train_type': 'segmentation', 'train_data': 'coco2017'},\n",
    "    \n",
    "    {'model': 'deeplabv3_resnet101', 'model_display_name': 'DeepLabV3-Resnet101',\n",
    "     'model_type': 'detection', 'train_type': 'detection', 'train_data': 'coco2017'},\n",
    "    \n",
    "    {'model': 'fcn_resnet101', 'model_display_name': 'FCN-Resnet101', 'model_type': 'detection', \n",
    "     'train_type': 'detection', 'train_data': 'coco2017'},\n",
    "    \n",
    "    {'model': 'r3d_18', 'model_display_name': 'R3D-18', 'model_type': 'video', \n",
    "     'train_type': 'video', 'train_data': 'kinetics400'},\n",
    "    \n",
    "    {'model': 'r2plus1d_18', 'model_display_name': 'R2Plus1D-18', 'model_type': 'video', \n",
    "     'train_type': 'video', 'train_data': 'kinetics400'},\n",
    "    \n",
    "    {'model': 'mc3_18', 'model_display_name': 'MC3-18','model_type': 'video', \n",
    "     'train_type': 'video', 'train_data': 'kinetics400'}\n",
    "]\n",
    "\n",
    "\n",
    "train_type_text = {\n",
    "    'imagenet': 'image classification',\n",
    "    'inception': 'image classification',\n",
    "    'video': 'video classification',\n",
    "    'segmentation': 'image segmentation',\n",
    "    'detection': 'keypoint detection'\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "    'kinetics400': 'Kinetics400',\n",
    "    'coco2017': 'CoCo2017',\n",
    "}\n",
    "\n",
    "torchvision_dictlist_ = copy(torchvision_dictlist)\n",
    "for i, dict_i in enumerate(torchvision_dictlist):\n",
    "    dict_i_random = copy(dict_i)\n",
    "    dict_i_random['train_data'] = None\n",
    "    dict_i_random['train_type'] = 'random'\n",
    "    dict_i_random['task_cluster'] = 'Random'\n",
    "    torchvision_dictlist_.append(dict_i_random)\n",
    "torchvision_dictlist = torchvision_dictlist_\n",
    "    \n",
    "    \n",
    "for i, dict_i in enumerate(torchvision_dictlist):\n",
    "    dict_i = torchvision_dictlist[i]\n",
    "    if dict_i['train_type'] != 'random':\n",
    "        dict_i['description'] = '{} trained on {} with the {} dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                               train_type_text[dict_i['train_type']],\n",
    "                                                                               train_data_text[dict_i['train_data']])\n",
    "    if dict_i['train_type'] == 'random':\n",
    "        dict_i['description'] = '{} randomly initialized, with no training.'.format(dict_i['model_display_name'])\n",
    "    \n",
    "torchvision_df = pd.DataFrame(torchvision_dictlist)\n",
    "torchvision_df['task_cluster'] = 'Semantic'\n",
    "torchvision_df['model_class'] = 'Convolutional'\n",
    "torchvision_df['model_source'] = 'torchvision'\n",
    "torchvision_df['model_source_url'] = 'pytorch.org/docs/stable/torchvision/models.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>vgg13_bn</td>\n",
       "      <td>VGG13-BatchNorm</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>random</td>\n",
       "      <td>None</td>\n",
       "      <td>VGG13-BatchNorm randomly initialized, with no ...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mnasnet1_0</td>\n",
       "      <td>MNASNet1.0</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>random</td>\n",
       "      <td>None</td>\n",
       "      <td>MNASNet1.0 randomly initialized, with no train...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>densenet201</td>\n",
       "      <td>DenseNet201</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>DenseNet201 trained on image classification wi...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model model_display_name model_type train_type train_data  \\\n",
       "45     vgg13_bn    VGG13-BatchNorm   imagenet     random       None   \n",
       "68   mnasnet1_0         MNASNet1.0   imagenet     random       None   \n",
       "19  densenet201        DenseNet201   imagenet   imagenet   imagenet   \n",
       "\n",
       "                                          description task_cluster  \\\n",
       "45  VGG13-BatchNorm randomly initialized, with no ...     Semantic   \n",
       "68  MNASNet1.0 randomly initialized, with no train...     Semantic   \n",
       "19  DenseNet201 trained on image classification wi...     Semantic   \n",
       "\n",
       "      model_class model_source  \\\n",
       "45  Convolutional  torchvision   \n",
       "68  Convolutional  torchvision   \n",
       "19  Convolutional  torchvision   \n",
       "\n",
       "                                   model_source_url  \n",
       "45  pytorch.org/docs/stable/torchvision/models.html  \n",
       "68  pytorch.org/docs/stable/torchvision/models.html  \n",
       "19  pytorch.org/docs/stable/torchvision/models.html  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskonomy_dictlist = [\n",
    "    \n",
    "    {'model': 'autoencoding', 'model_display_name': 'Autoencoder', 'task_cluster': '2D',\n",
    "        'description': 'Image compression and decompression'},\n",
    "    \n",
    "    {'model': 'class_object', 'model_display_name': 'Object Classification', 'task_cluster': 'Semantic',\n",
    "        'description': '1000-way object classification (via knowledge distillation from ImageNet).'},\n",
    "    \n",
    "    {'model': 'class_scene', 'model_display_name': 'Scene Classification', 'task_cluster': 'Semantic',\n",
    "        'description': 'Scene Classification (via knowledge distillation from MIT Places).'},\n",
    "    \n",
    "    {'model': 'curvature', 'model_display_name': 'Curvatures', 'task_cluster': '3D',\n",
    "        'description': 'Magnitude of 3D principal curvatures'},\n",
    "    \n",
    "    {'model': 'colorization', 'model_display_name': 'Colorization', 'task_cluster': None,\n",
    "        'description': 'Colorizing input grayscale images.'},\n",
    "    \n",
    "    {'model': 'denoising', 'model_display_name': 'Denoising', 'task_cluster': 'Other',\n",
    "        'description': 'Uncorrupted version of corrupted image.'},\n",
    "    \n",
    "    {'model': 'depth_euclidean', 'model_display_name': 'Euclidean Depth', 'task_cluster': '3D',\n",
    "        'description': 'Depth estimation'},\n",
    "    \n",
    "    {'model': 'depth_zbuffer', 'model_display_name': 'Z-Buffer Depth', 'task_cluster': '3D',\n",
    "        'description': 'Depth estimation.'},\n",
    "    \n",
    "    {'model': 'edge_occlusion', 'model_display_name': 'Occlusion Edges', 'task_cluster': '3D',\n",
    "        'description': 'Edges which include parts of the scene.'},\n",
    "    \n",
    "    {'model': 'edge_texture', 'model_display_name': 'Texture Edges', 'task_cluster': '2D',\n",
    "        'description': 'Edges computed from RGB only (texture edges).'},\n",
    "    \n",
    "    {'model': 'egomotion', 'model_display_name': 'Egomotion', 'task_cluster': 'Geometric',\n",
    "        'description': 'Odometry (camera poses) given three input images.'},\n",
    "    \n",
    "    {'model': 'fixated_pose', 'model_display_name': 'Camera Pose (Fixated)', 'task_cluster': 'Geometric',\n",
    "        'description': 'Relative camera pose with matching optical centers.'},\n",
    "    \n",
    "    {'model': 'inpainting', 'model_display_name': 'Inpainting', 'task_cluster': '2D',\n",
    "        'description': 'Filling in masked center of image.'},\n",
    "    \n",
    "    {'model': 'jigsaw', 'model_display_name': 'Jigsaw', 'task_cluster': 'Geometric',\n",
    "        'description': 'Putting scrambled image pieces back together.'},\n",
    "    \n",
    "    {'model': 'keypoints2d', 'model_display_name': '2D Keypoints', 'task_cluster': '2D',\n",
    "        'description': 'Keypoint estimation from RGB-only (texture features).'},\n",
    "    \n",
    "    {'model': 'keypoints3d', 'model_display_name': '3D Keypoints', 'task_cluster': '3D',\n",
    "        'description': '3D Keypoint estimation from underlying scene 3D.'},\n",
    "    \n",
    "    {'model': 'nonfixated_pose', 'model_display_name': 'Camera Pose (Nonfixated)', 'task_cluster': 'Geometric',\n",
    "        'description': 'Relative camera pose with distinct optical centers.'},\n",
    "    \n",
    "    {'model': 'normal', 'model_display_name': 'Surface Normals', 'task_cluster': 'Other',\n",
    "        'description': 'Pixel-wise surface normals.'},\n",
    "    \n",
    "    {'model': 'point_matching', 'model_display_name': 'Point Matching', 'task_cluster': 'Geometric',\n",
    "        'description': 'Classifying if centers of two images match or not.'},\n",
    "    \n",
    "    {'model': 'reshading', 'model_display_name': 'Reshading', 'task_cluster': '3D',\n",
    "        'description': 'Reshading with new lighting placed at camera location.'},\n",
    "    \n",
    "    {'model': 'room_layout', 'model_display_name': 'Room Layout', 'task_cluster': 'Geometric',\n",
    "        'description': 'Orientation and aspect ratio of cubic room layout.'},\n",
    "    \n",
    "    {'model': 'segment_semantic', 'model_display_name': 'Semantic Segmentation', 'task_cluster': 'Semantic',\n",
    "        'description': 'Pixel-wise semantic labeling (via knowledge distillation from MS COCO).'},\n",
    "    \n",
    "    {'model': 'segment_unsup25d', 'model_display_name': 'Unsupervised 2.5D Segmentation', 'task_cluster': '3D',\n",
    "        'description': 'Segmentation (graph cut approximation) on RGB-D-Normals-Curvature image.'},\n",
    "    \n",
    "    {'model': 'segment_unsup2d', 'model_display_name': 'Unsupervised 2D Segmentation', 'task_cluster': '2D',\n",
    "        'description': 'Segmentation (graph cut approximation) on RGB.'},\n",
    "    \n",
    "    {'model': 'vanishing_point', 'model_display_name': 'Vanishing Point', 'task_cluster': 'Geometric',\n",
    "        'description': 'Three Manhattan-world vanishing points.'}\n",
    "]\n",
    "\n",
    "taskonomy_df = pd.DataFrame(taskonomy_dictlist)\n",
    "problematic_models = ['colorization']\n",
    "taskonomy_df = taskonomy_df[~taskonomy_df.model.isin(problematic_models)]\n",
    "taskonomy_df['model_type'] = 'taskonomy'\n",
    "taskonomy_df['train_type'] = 'taskonomy'\n",
    "taskonomy_df['train_data'] = 'taskonomy'\n",
    "taskonomy_df['model_class'] = 'Convolutional'\n",
    "\n",
    "task_random = {'model': 'random_weights', 'model_display_name': 'Random Weights', \n",
    "               'task_cluster': 'Random', 'description': 'Taskonomy architecture randomly initialized.',\n",
    "               'model_type': 'taskonomy', 'train_type': 'taskonomy', 'train_data': None}\n",
    "\n",
    "task_random_df = pd.DataFrame(task_random,  index = [taskonomy_df.shape[0] + 1])\n",
    "\n",
    "taskonomy_df = pd.concat([taskonomy_df, task_random_df])\n",
    "\n",
    "taskonomy_df['model_source'] = 'taskonomy'\n",
    "taskonomy_df['model_source_url'] = 'github.com/alexsax/midlevel-reps/tree/visualpriors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>description</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>segment_unsup2d</td>\n",
       "      <td>Unsupervised 2D Segmentation</td>\n",
       "      <td>2D</td>\n",
       "      <td>Segmentation (graph cut approximation) on RGB.</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>reshading</td>\n",
       "      <td>Reshading</td>\n",
       "      <td>3D</td>\n",
       "      <td>Reshading with new lighting placed at camera l...</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jigsaw</td>\n",
       "      <td>Jigsaw</td>\n",
       "      <td>Geometric</td>\n",
       "      <td>Putting scrambled image pieces back together.</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model            model_display_name task_cluster  \\\n",
       "23  segment_unsup2d  Unsupervised 2D Segmentation           2D   \n",
       "19        reshading                     Reshading           3D   \n",
       "13           jigsaw                        Jigsaw    Geometric   \n",
       "\n",
       "                                          description model_type train_type  \\\n",
       "23     Segmentation (graph cut approximation) on RGB.  taskonomy  taskonomy   \n",
       "19  Reshading with new lighting placed at camera l...  taskonomy  taskonomy   \n",
       "13      Putting scrambled image pieces back together.  taskonomy  taskonomy   \n",
       "\n",
       "   train_data    model_class model_source  \\\n",
       "23  taskonomy  Convolutional    taskonomy   \n",
       "19  taskonomy  Convolutional    taskonomy   \n",
       "13  taskonomy  Convolutional    taskonomy   \n",
       "\n",
       "                                     model_source_url  \n",
       "23  github.com/alexsax/midlevel-reps/tree/visualpr...  \n",
       "19  github.com/alexsax/midlevel-reps/tree/visualpr...  \n",
       "13  github.com/alexsax/midlevel-reps/tree/visualpr...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taskonomy_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_dictlist = [\n",
    "    \n",
    "    {'model': 'cait_s24_224', 'model_display_name': 'CaIT-S24', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'coat_lite_mini', 'model_display_name': 'CoaT-Lite-Mini', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'coat_mini', 'model_display_name': 'CoaT-Mini', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'convit_base', 'model_display_name': 'ConViT-B', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'convit_small', 'model_display_name': 'ConViT-S', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'cspdarknet53', 'model_display_name': 'CSP-DarkNet53', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'cspresnet50', 'model_display_name': 'CSP-ResNet50', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'dla34', 'model_display_name': 'DLA34', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'dla169', 'model_display_name': 'DLA169', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'eca_nfnet_l0', 'model_display_name': 'ECA-NFNeT-L0', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'eca_nfnet_l1', 'model_display_name': 'ECA-NFNeT-L1', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'ecaresnet50d', 'model_display_name': 'ECA-Resnet50-D', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'ecaresnet101d', 'model_display_name': 'ECA-Resnet101-D', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'efficientnetv2_rw_s', 'model_display_name': 'EfficientNet-V2-S', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'fbnetc_100', 'model_display_name': 'FBNetC100', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'gernet_l', 'model_display_name': 'GerNet-L', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'gernet_s', 'model_display_name': 'GerNet-S', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'ghostnet_100', 'model_display_name': 'GhostNet100', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'hardcorenas_a', 'model_display_name': 'HardCoreNAS-A', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'hardcorenas_f', 'model_display_name': 'HardCoreNAS-F', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'levit_128', 'model_display_name': 'LeViT128', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'levit_256', 'model_display_name': 'LeViT256', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'inception_resnet_v2', 'model_display_name': 'Inception-Resnet-V2', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'inception_v3', 'model_display_name': 'Inception-V3', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'inception_v4', 'model_display_name': 'Inception-V4', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'inception_v4', 'model_display_name': 'Inception-V4', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mixer_b16_224', 'model_display_name': 'MLP-Mixer-B16', 'model_class': 'MLP-Mixer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'mixer_l16_224', 'model_display_name': 'MLP-Mixer-L16', 'model_class': 'MLP-Mixer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mixnet_l', 'model_display_name': 'MixNet-L', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mixnet_s', 'model_display_name': 'MixNet-S', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mnasnet_100', 'model_display_name': 'MNASNet100', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mnasnet_100', 'model_display_name': 'MNASNet100', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mobilenetv3_rw', 'model_display_name': 'MobileNet-V3', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'nasnetalarge', 'model_display_name': 'NASNet-A-Large', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'nf_resnet50', 'model_display_name': 'NF-ResNet50', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'nfnet_l0', 'model_display_name': 'NF-Net-L0', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'pit_b_224', 'model_display_name': 'PiT-B-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'pit_s_224', 'model_display_name': 'PiT-S-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'pit_ti_224', 'model_display_name': 'PiT-Ti-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'pnasnet5large', 'model_display_name': 'PNASNet-5-Large', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'regnetx_064', 'model_display_name': 'RegNetX-64', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'regnety_064', 'model_display_name': 'RegNetY-64', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'repvgg_b3', 'model_display_name': 'RepVGG-B3', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'repvgg_b3g4', 'model_display_name': 'RepVGG-B3G4', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'res2net50_26w_4s', 'model_display_name': 'Res2Net50-26W-4S', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnest50d', 'model_display_name': 'ResNest50D', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnetrs50', 'model_display_name': 'ResNetRS50', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'rexnet_100', 'model_display_name': 'RexNet100', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'selecsls60', 'model_display_name': 'SelecSLS60', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'semnasnet_100', 'model_display_name': 'SemNASNet100', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'seresnet152d', 'model_display_name': 'SEResNet152D', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'seresnext50_32x4d', 'model_display_name': 'SEResNext50-32x4D', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'skresnet18', 'model_display_name': 'SKResNet18', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'skresnext50_32x4d', 'model_display_name': 'SKResNext50-32x4D', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'spnasnet_100', 'model_display_name': 'SPNasNet100', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'swin_base_patch4_window7_224', 'model_display_name': 'Swin-B-P4-W7-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'swin_large_patch4_window7_224', 'model_display_name': 'Swin-L-P4-W7-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'swin_small_patch4_window7_224', 'model_display_name': 'Swin-S-P4-W7-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'swin_tiny_patch4_window7_224', 'model_display_name': 'Swin-T-P4-W7-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b1', 'model_display_name': 'EfficientNet-B1', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b3', 'model_display_name': 'EfficientNet-B3', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b5', 'model_display_name': 'EfficientNet-B5', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b7', 'model_display_name': 'EfficientNet-B7', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'tnt_s_patch16_224', 'model_display_name': 'TnT-P16-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'tresnet_l', 'model_display_name': 'TResNet-L', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'twins_pcpvt_base', 'model_display_name': 'Twins-PCPVT-B', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'twins_svt_base', 'model_display_name': 'Twins-SVT-B', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'visformer_small', 'model_display_name': 'Visformer', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'deit_base_patch16_224', 'model_display_name': 'DeiT-B-P16-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'deit_small_patch16_224', 'model_display_name': 'DeiT-S-P16-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'deit_tiny_patch16_224', 'model_display_name': 'DeiT-Ti-P16-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vit_large_patch16_224', 'model_display_name': 'ViT-L-P16-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vit_small_patch16_224', 'model_display_name': 'ViT-S-P16-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vit_tiny_patch16_224', 'model_display_name': 'ViT-T-P16-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vit_tiny_r_s16_p8_224', 'model_display_name': 'ViT-T-P8-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vit_base_patch16_224', 'model_display_name': 'ViT-B-P16-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vit_base_patch32_224', 'model_display_name': 'ViT-B-P32-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vit_small_patch32_224', 'model_display_name': 'ViT-S-P32-224', 'model_class': 'Transformer',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'xception', 'model_display_name': 'XCeption', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'xception65', 'model_display_name': 'XCeption65', 'model_class': 'Convolutional',\n",
    "     'model_type': 'imagenet', 'train_type': 'imagenet', 'train_data': 'imagenet'},\n",
    "    \n",
    "]\n",
    "\n",
    "train_type_text = {\n",
    "    'imagenet': 'image classification',\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "}\n",
    "\n",
    "timm_dictlist_ = copy(timm_dictlist)\n",
    "for i, dict_i in enumerate(timm_dictlist):\n",
    "    dict_i_random = copy(dict_i)\n",
    "    dict_i_random['train_data'] = None\n",
    "    dict_i_random['train_type'] = 'random'\n",
    "    dict_i_random['task_cluster'] = 'Random'\n",
    "    timm_dictlist_.append(dict_i_random)\n",
    "timm_dictlist = timm_dictlist_\n",
    "    \n",
    "    \n",
    "for i, dict_i in enumerate(timm_dictlist):\n",
    "    dict_i = timm_dictlist[i]\n",
    "    if dict_i['train_type'] != 'random':\n",
    "        dict_i['description'] = '{} trained on {} with the {} dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                               train_type_text[dict_i['train_type']],\n",
    "                                                                               train_data_text[dict_i['train_data']])\n",
    "    if dict_i['train_type'] == 'random':\n",
    "        dict_i['description'] = '{} randomly initialized, with no training.'.format(dict_i['model_display_name'])\n",
    "    \n",
    "timm_df = pd.DataFrame(timm_dictlist)\n",
    "timm_df['task_cluster'] = 'Semantic'\n",
    "timm_df['model_source'] = 'timm'\n",
    "timm_df['model_source_url'] = 'https://github.com/rwightman/pytorch-image-models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>xception</td>\n",
       "      <td>XCeption</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>XCeption trained on image classification with ...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>nasnetalarge</td>\n",
       "      <td>NASNet-A-Large</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>NASNet-A-Large trained on image classification...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>swin_tiny_patch4_window7_224</td>\n",
       "      <td>Swin-T-P4-W7-224</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>random</td>\n",
       "      <td>None</td>\n",
       "      <td>Swin-T-P4-W7-224 randomly initialized, with no...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model model_display_name    model_class  \\\n",
       "78                       xception           XCeption  Convolutional   \n",
       "33                   nasnetalarge     NASNet-A-Large  Convolutional   \n",
       "138  swin_tiny_patch4_window7_224   Swin-T-P4-W7-224    Transformer   \n",
       "\n",
       "    model_type train_type train_data  \\\n",
       "78    imagenet   imagenet   imagenet   \n",
       "33    imagenet   imagenet   imagenet   \n",
       "138   imagenet     random       None   \n",
       "\n",
       "                                           description task_cluster  \\\n",
       "78   XCeption trained on image classification with ...     Semantic   \n",
       "33   NASNet-A-Large trained on image classification...     Semantic   \n",
       "138  Swin-T-P4-W7-224 randomly initialized, with no...     Semantic   \n",
       "\n",
       "    model_source                                   model_source_url  \n",
       "78          timm  https://github.com/rwightman/pytorch-image-mod...  \n",
       "33          timm  https://github.com/rwightman/pytorch-image-mod...  \n",
       "138         timm  https://github.com/rwightman/pytorch-image-mod...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alexnet</td>\n",
       "      <td>AlexNet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>AlexNet trained on image classification with t...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vgg11</td>\n",
       "      <td>VGG11</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>VGG11 trained on image classification with the...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vgg13</td>\n",
       "      <td>VGG13</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>VGG13 trained on image classification with the...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vgg16</td>\n",
       "      <td>VGG16</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>VGG16 trained on image classification with the...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vgg19</td>\n",
       "      <td>VGG19</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>VGG19 trained on image classification with the...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>segment_semantic</td>\n",
       "      <td>Semantic Segmentation</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Pixel-wise semantic labeling (via knowledge di...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>segment_unsup25d</td>\n",
       "      <td>Unsupervised 2.5D Segmentation</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Segmentation (graph cut approximation) on RGB-...</td>\n",
       "      <td>3D</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>segment_unsup2d</td>\n",
       "      <td>Unsupervised 2D Segmentation</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Segmentation (graph cut approximation) on RGB.</td>\n",
       "      <td>2D</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>vanishing_point</td>\n",
       "      <td>Vanishing Point</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Three Manhattan-world vanishing points.</td>\n",
       "      <td>Geometric</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>random_weights</td>\n",
       "      <td>Random Weights</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>None</td>\n",
       "      <td>Taskonomy architecture randomly initialized.</td>\n",
       "      <td>Random</td>\n",
       "      <td>NaN</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               model              model_display_name model_type train_type  \\\n",
       "0            alexnet                         AlexNet   imagenet   imagenet   \n",
       "1              vgg11                           VGG11   imagenet   imagenet   \n",
       "2              vgg13                           VGG13   imagenet   imagenet   \n",
       "3              vgg16                           VGG16   imagenet   imagenet   \n",
       "4              vgg19                           VGG19   imagenet   imagenet   \n",
       "..               ...                             ...        ...        ...   \n",
       "21  segment_semantic           Semantic Segmentation  taskonomy  taskonomy   \n",
       "22  segment_unsup25d  Unsupervised 2.5D Segmentation  taskonomy  taskonomy   \n",
       "23   segment_unsup2d    Unsupervised 2D Segmentation  taskonomy  taskonomy   \n",
       "24   vanishing_point                 Vanishing Point  taskonomy  taskonomy   \n",
       "25    random_weights                  Random Weights  taskonomy  taskonomy   \n",
       "\n",
       "   train_data                                        description task_cluster  \\\n",
       "0    imagenet  AlexNet trained on image classification with t...     Semantic   \n",
       "1    imagenet  VGG11 trained on image classification with the...     Semantic   \n",
       "2    imagenet  VGG13 trained on image classification with the...     Semantic   \n",
       "3    imagenet  VGG16 trained on image classification with the...     Semantic   \n",
       "4    imagenet  VGG19 trained on image classification with the...     Semantic   \n",
       "..        ...                                                ...          ...   \n",
       "21  taskonomy  Pixel-wise semantic labeling (via knowledge di...     Semantic   \n",
       "22  taskonomy  Segmentation (graph cut approximation) on RGB-...           3D   \n",
       "23  taskonomy     Segmentation (graph cut approximation) on RGB.           2D   \n",
       "24  taskonomy            Three Manhattan-world vanishing points.    Geometric   \n",
       "25       None       Taskonomy architecture randomly initialized.       Random   \n",
       "\n",
       "      model_class model_source  \\\n",
       "0   Convolutional  torchvision   \n",
       "1   Convolutional  torchvision   \n",
       "2   Convolutional  torchvision   \n",
       "3   Convolutional  torchvision   \n",
       "4   Convolutional  torchvision   \n",
       "..            ...          ...   \n",
       "21  Convolutional    taskonomy   \n",
       "22  Convolutional    taskonomy   \n",
       "23  Convolutional    taskonomy   \n",
       "24  Convolutional    taskonomy   \n",
       "25            NaN    taskonomy   \n",
       "\n",
       "                                     model_source_url  \n",
       "0     pytorch.org/docs/stable/torchvision/models.html  \n",
       "1     pytorch.org/docs/stable/torchvision/models.html  \n",
       "2     pytorch.org/docs/stable/torchvision/models.html  \n",
       "3     pytorch.org/docs/stable/torchvision/models.html  \n",
       "4     pytorch.org/docs/stable/torchvision/models.html  \n",
       "..                                                ...  \n",
       "21  github.com/alexsax/midlevel-reps/tree/visualpr...  \n",
       "22  github.com/alexsax/midlevel-reps/tree/visualpr...  \n",
       "23  github.com/alexsax/midlevel-reps/tree/visualpr...  \n",
       "24  github.com/alexsax/midlevel-reps/tree/visualpr...  \n",
       "25  github.com/alexsax/midlevel-reps/tree/visualpr...  \n",
       "\n",
       "[103 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([torchvision_df, taskonomy_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([torchvision_df, taskonomy_df, timm_df]).to_csv('model_typology.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
