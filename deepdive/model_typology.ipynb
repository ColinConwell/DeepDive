{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Torchvision Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision_dictlist = [\n",
    "    \n",
    "    {'model': 'alexnet', 'model_display_name': 'AlexNet', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'vgg11', 'model_display_name': 'VGG11', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg13', 'model_display_name': 'VGG13', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg16', 'model_display_name': 'VGG16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg19', 'model_display_name': 'VGG19', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg11_bn', 'model_display_name': 'VGG11-BatchNorm', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg13_bn', 'model_display_name': 'VGG13-BatchNorm', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg16_bn', 'model_display_name': 'VGG16-BatchNorm', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'vgg19_bn', 'model_display_name': 'VGG19-BatchNorm', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'resnet18', 'model_display_name': 'ResNet18', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'resnet34', 'model_display_name': 'ResNet34', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnet50', 'model_display_name': 'ResNet50', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnet101', 'model_display_name': 'ResNet101', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnet152', 'model_display_name': 'ResNet152', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'squeezenet1_0', 'model_display_name': 'SqueezeNet1.0', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'squeezenet1_1', 'model_display_name': 'SqueezeNet1.1', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet121', 'model_display_name': 'DenseNet121', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet161', 'model_display_name': 'DenseNet161', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet169', 'model_display_name': 'DenseNet169', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'densenet201', 'model_display_name': 'DenseNet201', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'googlenet', 'model_display_name': 'GoogleNet', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "\n",
    "    {'model': 'shufflenet_v2_x0_5', 'model_display_name': 'ShuffleNet-V2-x0.5', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    " \n",
    "    {'model': 'shufflenet_v2_x1_0', 'model_display_name': 'ShuffleNet-V2-x1.0', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mobilenet_v2', 'model_display_name': 'MobileNet-V2', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnext50_32x4d', 'model_display_name': 'ResNext50-32x4D', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'resnext101_32x8d', 'model_display_name': 'ResNext50-32x8D', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'wide_resnet50_2', 'model_display_name': 'Wide-ResNet50', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'wide_resnet101_2', 'model_display_name': 'Wide-ResNet101', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mnasnet0_5', 'model_display_name': 'MNASNet0.5', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "    \n",
    "    {'model': 'mnasnet1_0', 'model_display_name': 'MNASNet1.0', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet'},\n",
    "]\n",
    "\n",
    "\n",
    "train_type_text = {\n",
    "    'classification': 'image classification',\n",
    "    'video': 'video classification',\n",
    "    'segmentation': 'image segmentation',\n",
    "    'detection': 'keypoint detection'\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "    'kinetics400': 'Kinetics400',\n",
    "    'coco2017': 'CoCo2017',\n",
    "}\n",
    "\n",
    "torchvision_dictlist_ = copy(torchvision_dictlist)\n",
    "for i, dict_i in enumerate(torchvision_dictlist):\n",
    "    dict_i_random = copy(dict_i)\n",
    "    dict_i_random['train_data'] = None\n",
    "    dict_i_random['train_type'] = 'random'\n",
    "    dict_i_random['task_cluster'] = 'Random'\n",
    "    torchvision_dictlist_.append(dict_i_random)\n",
    "torchvision_dictlist = torchvision_dictlist_\n",
    "    \n",
    "    \n",
    "for i, dict_i in enumerate(torchvision_dictlist):\n",
    "    dict_i = torchvision_dictlist[i]\n",
    "    if dict_i['train_type'] != 'random':\n",
    "        dict_i['description'] = '{} trained on {} with the {} dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                               train_type_text[dict_i['train_type']],\n",
    "                                                                               train_data_text[dict_i['train_data']])\n",
    "    if dict_i['train_type'] == 'random':\n",
    "        dict_i['description'] = '{} randomly initialized, with no training.'.format(dict_i['model_display_name'])\n",
    "    \n",
    "torchvision_df = pd.DataFrame(torchvision_dictlist)\n",
    "torchvision_df['architecture'] = torchvision_df['model']\n",
    "torchvision_df['task_cluster'] = 'Semantic'\n",
    "torchvision_df['model_class'] = 'Convolutional'\n",
    "torchvision_df['model_source'] = 'torchvision'\n",
    "torchvision_df['model_source_url'] = 'pytorch.org/docs/stable/torchvision/models.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vgg16</td>\n",
       "      <td>VGG16</td>\n",
       "      <td>classification</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>VGG16 trained on image classification with the...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>vgg16</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>vgg19_bn</td>\n",
       "      <td>VGG19-BatchNorm</td>\n",
       "      <td>random</td>\n",
       "      <td>None</td>\n",
       "      <td>VGG19-BatchNorm randomly initialized, with no ...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>vgg19_bn</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>vgg16_bn</td>\n",
       "      <td>VGG16-BatchNorm</td>\n",
       "      <td>random</td>\n",
       "      <td>None</td>\n",
       "      <td>VGG16-BatchNorm randomly initialized, with no ...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>vgg16_bn</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>torchvision</td>\n",
       "      <td>pytorch.org/docs/stable/torchvision/models.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model model_display_name      train_type train_data  \\\n",
       "3      vgg16              VGG16  classification   imagenet   \n",
       "38  vgg19_bn    VGG19-BatchNorm          random       None   \n",
       "37  vgg16_bn    VGG16-BatchNorm          random       None   \n",
       "\n",
       "                                          description task_cluster  \\\n",
       "3   VGG16 trained on image classification with the...     Semantic   \n",
       "38  VGG19-BatchNorm randomly initialized, with no ...     Semantic   \n",
       "37  VGG16-BatchNorm randomly initialized, with no ...     Semantic   \n",
       "\n",
       "   architecture    model_class model_source  \\\n",
       "3         vgg16  Convolutional  torchvision   \n",
       "38     vgg19_bn  Convolutional  torchvision   \n",
       "37     vgg16_bn  Convolutional  torchvision   \n",
       "\n",
       "                                   model_source_url  \n",
       "3   pytorch.org/docs/stable/torchvision/models.html  \n",
       "38  pytorch.org/docs/stable/torchvision/models.html  \n",
       "37  pytorch.org/docs/stable/torchvision/models.html  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Timm Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_dictlist = [\n",
    "    \n",
    "    {'model': 'botnet26t_256', 'model_display_name': 'BotNet-26T',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'cait_s24_224', 'model_display_name': 'CaiT-S24',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'cait_xxs24_224', 'model_display_name': 'CaiT-XXS24',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'coat_lite_tiny', 'model_display_name': 'CoaT-Lite-Tiny',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'coat_lite_small', 'model_display_name': 'CoaT-Lite-Small',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'coat_mini', 'model_display_name': 'CoaT-Mini', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'convit_base', 'model_display_name': 'ConViT-B',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Hybrid'},\n",
    "    \n",
    "    {'model': 'convit_small', 'model_display_name': 'ConViT-S',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Hybrid'},\n",
    "    \n",
    "    {'model': 'convit_tiny', 'model_display_name': 'ConViT-T', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Hybrid'},\n",
    "    \n",
    "    {'model': 'convmixer_768_32', 'model_display_name': 'ConvMixer-768-32', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Hybrid'},\n",
    "    \n",
    "    {'model': 'convnext_base', 'model_display_name': 'ConvNext-B',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'convnext_large', 'model_display_name': 'ConvNext-L', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'convnext_small', 'model_display_name': 'ConvNext-S', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'crossvit_base_240', 'model_display_name': 'CrossViT-B',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'crossvit_tiny_240', 'model_display_name': 'CrossViT-T',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'deit_base_patch16_224', 'model_display_name': 'DeiT-B-P16-224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'deit_small_patch16_224', 'model_display_name': 'DeiT-S-P16-224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'deit_tiny_patch16_224', 'model_display_name': 'DeiT-T-P16-224', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'cspdarknet53', 'model_display_name': 'CSP-DarkNet53', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'cspresnet50', 'model_display_name': 'CSP-ResNet50',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'dla34', 'model_display_name': 'DLA34',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'dla169', 'model_display_name': 'DLA169',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'eca_nfnet_l0', 'model_display_name': 'ECA-NFNeT-L0',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'eca_nfnet_l1', 'model_display_name': 'ECA-NFNeT-L1', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'ecaresnet50d', 'model_display_name': 'ECA-Resnet50-D', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'ecaresnet101d', 'model_display_name': 'ECA-Resnet101-D',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'efficientnet_b1', 'model_display_name': 'EfficientNet-B1',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'efficientnet_b3', 'model_display_name': 'EfficientNet-B3',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'efficientnet_b5', 'model_display_name': 'EfficientNet-B5',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'efficientnet_b7', 'model_display_name': 'EfficientNet-B7',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'gcresnet50t', 'model_display_name': 'GCResNet50T',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'efficientnetv2_rw_s', 'model_display_name': 'EfficientNet-V2-S',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'fbnetc_100', 'model_display_name': 'FBNetC100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'gernet_l', 'model_display_name': 'GerNet-L', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'gernet_s', 'model_display_name': 'GerNet-S',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'ghostnet_100', 'model_display_name': 'GhostNet100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'gmixer_24_224', 'model_display_name': 'GMixer-24', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'gmlp_s16_224', 'model_display_name': 'GMLP-S16',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'halonet26t', 'model_display_name': 'HaloNet-26T',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'hardcorenas_a', 'model_display_name': 'HardCoreNAS-A',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'hardcorenas_f', 'model_display_name': 'HardCoreNAS-F',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'hrnet_w18', 'model_display_name': 'HRNet-W18',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'hrnet_w64', 'model_display_name': 'HRNet-W64',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'jx_nest_base', 'model_display_name': 'JX-NesT-Base',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'jx_nest_small', 'model_display_name': 'JX-NesT-Small',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'jx_nest_tiny', 'model_display_name': 'JX-NesT-Tiny',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'lambda_resnet26t', 'model_display_name': 'Lambda-ResNet-26T',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'levit_128', 'model_display_name': 'LeViT128',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'levit_256', 'model_display_name': 'LeViT256',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'inception_resnet_v2', 'model_display_name': 'Inception-Resnet-V2',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'inception_v3', 'model_display_name': 'Inception-V3',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'inception_v4', 'model_display_name': 'Inception-V4',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'mixer_b16_224', 'model_display_name': 'MLP-Mixer-B16',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'MLP-Mixer'},\n",
    "\n",
    "    {'model': 'mixer_l16_224', 'model_display_name': 'MLP-Mixer-L16',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'mixnet_l', 'model_display_name': 'MixNet-L',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'mixnet_s', 'model_display_name': 'MixNet-S',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'mnasnet_100', 'model_display_name': 'MNASNet100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'mnasnet_100', 'model_display_name': 'MNASNet100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "        \n",
    "    {'model': 'mobilenetv3_large_100', 'model_display_name': 'MobileNet-V3-Large',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'mobilenetv3_rw', 'model_display_name': 'MobileNet-V3',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'mobilevit_s', 'model_display_name': 'MobileViT-S',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'nasnetalarge', 'model_display_name': 'NASNet-A-Large',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'nf_resnet50', 'model_display_name': 'NF-ResNet50',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'nfnet_l0', 'model_display_name': 'NF-Net-L0',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'pit_b_224', 'model_display_name': 'PiT-B-224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'pit_s_224', 'model_display_name': 'PiT-S-224', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'pit_ti_224', 'model_display_name': 'PiT-T-224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'pnasnet5large', 'model_display_name': 'PNASNet-5-Large',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'poolformer_s12', 'model_display_name': 'PoolFormer-S12',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'poolformer_s36', 'model_display_name': 'PoolFormer-S36',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'poolformer_m36', 'model_display_name': 'PoolFormer-M36',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'pit_ti_224', 'model_display_name': 'PiT-T-224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'regnetx_064', 'model_display_name': 'RegNetX-64',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'regnety_064', 'model_display_name': 'RegNetY-64',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'repvgg_b1', 'model_display_name': 'RepVGG-B1',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'repvgg_b1g4', 'model_display_name': 'RepVGG-B1G4',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'res2net50_26w_4s', 'model_display_name': 'Res2Net50-26W-4S',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'resmlp_12_224', 'model_display_name': 'ResMLP-12',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'resmlp_24_224', 'model_display_name': 'ResMLP-24',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'resmlp_36_224', 'model_display_name': 'ResMLP-36',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'resmlp_big_24_224', 'model_display_name': 'ResMLP-Big-24',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'resnest50d', 'model_display_name': 'ResNest50D',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'resnetrs50', 'model_display_name': 'ResNetRS50',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'rexnet_100', 'model_display_name': 'RexNet100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'selecsls60', 'model_display_name': 'SelecSLS60',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'semnasnet_100', 'model_display_name': 'SemNASNet100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'seresnet50', 'model_display_name': 'SEResNet50',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'seresnext50_32x4d', 'model_display_name': 'SEResNext50-32x4D',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'skresnet18', 'model_display_name': 'SKResNet18',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'skresnext50_32x4d', 'model_display_name': 'SKResNext50-32x4D',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'spnasnet_100', 'model_display_name': 'SPNasNet100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'swin_base_patch4_window7_224', 'model_display_name': 'Swin-B-P4-W7', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'swin_large_patch4_window7_224', 'model_display_name': 'Swin-L-P4-W7', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'swin_small_patch4_window7_224', 'model_display_name': 'Swin-S-P4-W7', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'swin_tiny_patch4_window7_224', 'model_display_name': 'Swin-T-P4-W7', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b0', 'model_display_name': 'TF-EfficientNet-B0',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b1', 'model_display_name': 'TF-EfficientNet-B1',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b2', 'model_display_name': 'TF-EfficientNet-B2',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b3', 'model_display_name': 'TF-EfficientNet-B3',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b4', 'model_display_name': 'TF-EfficientNet-B4',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b5', 'model_display_name': 'TF-EfficientNet-B5',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b6', 'model_display_name': 'TF-EfficientNet-B6',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tf_efficientnet_b7', 'model_display_name': 'TF-EfficientNet-B7',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'tnt_s_patch16_224', 'model_display_name': 'TnT-P16-224', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'tresnet_l', 'model_display_name': 'TResNet-L',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'twins_pcpvt_base', 'model_display_name': 'Twins-PCPVT-B', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'twins_svt_base', 'model_display_name': 'Twins-SVT-B', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'visformer_small', 'model_display_name': 'Visformer',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_large_patch16_224', 'model_display_name': 'ViT-L-P16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_small_patch16_224', 'model_display_name': 'ViT-S-P16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_tiny_patch16_224', 'model_display_name': 'ViT-T-P16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_base_patch16_224', 'model_display_name': 'ViT-B-P16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_base_patch32_224', 'model_display_name': 'ViT-B-P32', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_large_patch32_224', 'model_display_name': 'ViT-L-P32', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_small_patch32_224', 'model_display_name': 'ViT-S-P32', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'volo_d1_224', 'model_display_name': 'VoLo-D1', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'volo_d3_224', 'model_display_name': 'VoLo-D3', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'volo_d5_224', 'model_display_name': 'VoLo-D5', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'xception', 'model_display_name': 'XCeption',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'xception65', 'model_display_name': 'XCeption65',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'xcit_nano_12_p8_224', 'model_display_name': 'XCIT-N-12-P8', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'xcit_small_12_p8_224', 'model_display_name': 'XCIT-S-12-P8', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'xcit_medium_24_p8_224', 'model_display_name': 'XCIT-M-24-P8', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'xcit_nano_12_p16_224', 'model_display_name': 'XCIT-N-12-P16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'xcit_small_12_p16_224', 'model_display_name': 'XCIT-S-12-P16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'xcit_medium_24_p16_224', 'model_display_name': 'XCIT-M-24-P16', \n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "]\n",
    "\n",
    "train_type_text = {\n",
    "    'classification': 'image classification',\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "}\n",
    "\n",
    "timm_dictlist_ = copy(timm_dictlist)\n",
    "for i, dict_i in enumerate(timm_dictlist):\n",
    "    dict_i_random = copy(dict_i)\n",
    "    dict_i_random['train_data'] = None\n",
    "    dict_i_random['train_type'] = 'random'\n",
    "    dict_i_random['task_cluster'] = 'Random'\n",
    "    timm_dictlist_.append(dict_i_random)\n",
    "timm_dictlist = timm_dictlist_\n",
    "    \n",
    "    \n",
    "for i, dict_i in enumerate(timm_dictlist):\n",
    "    dict_i = timm_dictlist[i]\n",
    "    if dict_i['train_type'] != 'random':\n",
    "        dict_i['description'] = '{} trained on {} with the {} dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                               train_type_text[dict_i['train_type']],\n",
    "                                                                               train_data_text[dict_i['train_data']])\n",
    "    if dict_i['train_type'] == 'random':\n",
    "        dict_i['description'] = '{} randomly initialized, with no training.'.format(dict_i['model_display_name'])\n",
    "    \n",
    "timm_df = pd.DataFrame(timm_dictlist)\n",
    "timm_df['architecture'] = timm_df['model']\n",
    "timm_df['task_cluster'] = 'Semantic'\n",
    "timm_df['model_source'] = 'timm'\n",
    "timm_df['model_source_url'] = 'https://github.com/rwightman/pytorch-image-models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>model_class</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>lambda_resnet26t</td>\n",
       "      <td>Lambda-ResNet-26T</td>\n",
       "      <td>classification</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>Lambda-ResNet-26T trained on image classificat...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>lambda_resnet26t</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>swin_large_patch4_window7_224</td>\n",
       "      <td>Swin-L-P4-W7</td>\n",
       "      <td>random</td>\n",
       "      <td>None</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>Swin-L-P4-W7 randomly initialized, with no tra...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>swin_large_patch4_window7_224</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>convnext_small</td>\n",
       "      <td>ConvNext-S</td>\n",
       "      <td>classification</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>ConvNext-S trained on image classification wit...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>convnext_small</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             model model_display_name      train_type  \\\n",
       "46                lambda_resnet26t  Lambda-ResNet-26T  classification   \n",
       "218  swin_large_patch4_window7_224       Swin-L-P4-W7          random   \n",
       "12                  convnext_small         ConvNext-S  classification   \n",
       "\n",
       "    train_data    model_class  \\\n",
       "46    imagenet  Convolutional   \n",
       "218       None    Transformer   \n",
       "12    imagenet  Convolutional   \n",
       "\n",
       "                                           description task_cluster  \\\n",
       "46   Lambda-ResNet-26T trained on image classificat...     Semantic   \n",
       "218  Swin-L-P4-W7 randomly initialized, with no tra...     Semantic   \n",
       "12   ConvNext-S trained on image classification wit...     Semantic   \n",
       "\n",
       "                      architecture model_source  \\\n",
       "46                lambda_resnet26t         timm   \n",
       "218  swin_large_patch4_window7_224         timm   \n",
       "12                  convnext_small         timm   \n",
       "\n",
       "                                      model_source_url  \n",
       "46   https://github.com/rwightman/pytorch-image-mod...  \n",
       "218  https://github.com/rwightman/pytorch-image-mod...  \n",
       "12   https://github.com/rwightman/pytorch-image-mod...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Taskonomy Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskonomy_dictlist = [\n",
    "    \n",
    "    {'model': 'autoencoding', 'model_display_name': 'Autoencoder', 'task_cluster': '2D',\n",
    "        'description': 'Image compression and decompression'},\n",
    "    \n",
    "    {'model': 'class_object', 'model_display_name': 'Object Classification', 'task_cluster': 'Semantic',\n",
    "        'description': '1000-way object classification (via knowledge distillation from ImageNet).'},\n",
    "    \n",
    "    {'model': 'class_scene', 'model_display_name': 'Scene Classification', 'task_cluster': 'Semantic',\n",
    "        'description': 'Scene Classification (via knowledge distillation from MIT Places).'},\n",
    "    \n",
    "    {'model': 'curvature', 'model_display_name': 'Curvatures', 'task_cluster': '3D',\n",
    "        'description': 'Magnitude of 3D principal curvatures'},\n",
    "    \n",
    "    {'model': 'colorization', 'model_display_name': 'Colorization', 'task_cluster': None,\n",
    "        'description': 'Colorizing input grayscale images.'},\n",
    "    \n",
    "    {'model': 'denoising', 'model_display_name': 'Denoising', 'task_cluster': 'Other',\n",
    "        'description': 'Uncorrupted version of corrupted image.'},\n",
    "    \n",
    "    {'model': 'depth_euclidean', 'model_display_name': 'Euclidean Depth', 'task_cluster': '3D',\n",
    "        'description': 'Depth estimation'},\n",
    "    \n",
    "    {'model': 'depth_zbuffer', 'model_display_name': 'Z-Buffer Depth', 'task_cluster': '3D',\n",
    "        'description': 'Depth estimation.'},\n",
    "    \n",
    "    {'model': 'edge_occlusion', 'model_display_name': 'Occlusion Edges', 'task_cluster': '3D',\n",
    "        'description': 'Edges which include parts of the scene.'},\n",
    "    \n",
    "    {'model': 'edge_texture', 'model_display_name': 'Texture Edges', 'task_cluster': '2D',\n",
    "        'description': 'Edges computed from RGB only (texture edges).'},\n",
    "    \n",
    "    {'model': 'egomotion', 'model_display_name': 'Egomotion', 'task_cluster': 'Geometric',\n",
    "        'description': 'Odometry (camera poses) given three input images.'},\n",
    "    \n",
    "    {'model': 'fixated_pose', 'model_display_name': 'Camera Pose (Fixated)', 'task_cluster': 'Geometric',\n",
    "        'description': 'Relative camera pose with matching optical centers.'},\n",
    "    \n",
    "    {'model': 'inpainting', 'model_display_name': 'Inpainting', 'task_cluster': '2D',\n",
    "        'description': 'Filling in masked center of image.'},\n",
    "    \n",
    "    {'model': 'jigsaw', 'model_display_name': 'Jigsaw', 'task_cluster': 'Geometric',\n",
    "        'description': 'Putting scrambled image pieces back together.'},\n",
    "    \n",
    "    {'model': 'keypoints2d', 'model_display_name': '2D Keypoints', 'task_cluster': '2D',\n",
    "        'description': 'Keypoint estimation from RGB-only (texture features).'},\n",
    "    \n",
    "    {'model': 'keypoints3d', 'model_display_name': '3D Keypoints', 'task_cluster': '3D',\n",
    "        'description': '3D Keypoint estimation from underlying scene 3D.'},\n",
    "    \n",
    "    {'model': 'nonfixated_pose', 'model_display_name': 'Camera Pose (Nonfixated)', 'task_cluster': 'Geometric',\n",
    "        'description': 'Relative camera pose with distinct optical centers.'},\n",
    "    \n",
    "    {'model': 'normal', 'model_display_name': 'Surface Normals', 'task_cluster': '3D',\n",
    "        'description': 'Pixel-wise surface normals.'},\n",
    "    \n",
    "    {'model': 'point_matching', 'model_display_name': 'Point Matching', 'task_cluster': 'Geometric',\n",
    "        'description': 'Classifying if centers of two images match or not.'},\n",
    "    \n",
    "    {'model': 'reshading', 'model_display_name': 'Reshading', 'task_cluster': '3D',\n",
    "        'description': 'Reshading with new lighting placed at camera location.'},\n",
    "    \n",
    "    {'model': 'room_layout', 'model_display_name': 'Room Layout', 'task_cluster': 'Geometric',\n",
    "        'description': 'Orientation and aspect ratio of cubic room layout.'},\n",
    "    \n",
    "    {'model': 'segment_semantic', 'model_display_name': 'Semantic Segmentation', 'task_cluster': 'Semantic',\n",
    "        'description': 'Pixel-wise semantic labeling (via knowledge distillation from MS COCO).'},\n",
    "    \n",
    "    {'model': 'segment_unsup25d', 'model_display_name': 'Unsupervised 2.5D Segmentation', 'task_cluster': '3D',\n",
    "        'description': 'Segmentation (graph cut approximation) on RGB-D-Normals-Curvature image.'},\n",
    "    \n",
    "    {'model': 'segment_unsup2d', 'model_display_name': 'Unsupervised 2D Segmentation', 'task_cluster': '2D',\n",
    "        'description': 'Segmentation (graph cut approximation) on RGB.'},\n",
    "    \n",
    "    {'model': 'vanishing_point', 'model_display_name': 'Vanishing Point', 'task_cluster': 'Geometric',\n",
    "        'description': 'Three Manhattan-world vanishing points.'}\n",
    "]\n",
    "\n",
    "taskonomy_df = pd.DataFrame(taskonomy_dictlist)\n",
    "problematic_models = ['colorization']\n",
    "taskonomy_df = taskonomy_df[~taskonomy_df.model.isin(problematic_models)]\n",
    "taskonomy_df['architecture'] = 'resnet50'\n",
    "taskonomy_df['train_type'] = 'taskonomy'\n",
    "taskonomy_df['train_data'] = 'taskonomy'\n",
    "\n",
    "task_random = {'model': 'random_weights', 'model_display_name': 'Random Weights', \n",
    "               'architecture': 'resnet50', 'train_type': 'taskonomy', 'train_data': None, \n",
    "               'task_cluster': 'Random', 'description': 'Taskonomy architecture randomly initialized.'}\n",
    "\n",
    "task_random_df = pd.DataFrame(task_random,  index = [taskonomy_df.shape[0] + 1])\n",
    "\n",
    "taskonomy_df = pd.concat([taskonomy_df, task_random_df])\n",
    "\n",
    "taskonomy_df['model_class'] = 'Convolutional'\n",
    "taskonomy_df['model_source'] = 'taskonomy'\n",
    "taskonomy_df['model_source_url'] = 'github.com/alexsax/midlevel-reps/tree/visualpriors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>description</th>\n",
       "      <th>architecture</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>autoencoding</td>\n",
       "      <td>Autoencoder</td>\n",
       "      <td>2D</td>\n",
       "      <td>Image compression and decompression</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>edge_texture</td>\n",
       "      <td>Texture Edges</td>\n",
       "      <td>2D</td>\n",
       "      <td>Edges computed from RGB only (texture edges).</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>normal</td>\n",
       "      <td>Surface Normals</td>\n",
       "      <td>3D</td>\n",
       "      <td>Pixel-wise surface normals.</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model model_display_name task_cluster  \\\n",
       "0   autoencoding        Autoencoder           2D   \n",
       "9   edge_texture      Texture Edges           2D   \n",
       "17        normal    Surface Normals           3D   \n",
       "\n",
       "                                      description architecture train_type  \\\n",
       "0             Image compression and decompression     resnet50  taskonomy   \n",
       "9   Edges computed from RGB only (texture edges).     resnet50  taskonomy   \n",
       "17                    Pixel-wise surface normals.     resnet50  taskonomy   \n",
       "\n",
       "   train_data    model_class model_source  \\\n",
       "0   taskonomy  Convolutional    taskonomy   \n",
       "9   taskonomy  Convolutional    taskonomy   \n",
       "17  taskonomy  Convolutional    taskonomy   \n",
       "\n",
       "                                     model_source_url  \n",
       "0   github.com/alexsax/midlevel-reps/tree/visualpr...  \n",
       "9   github.com/alexsax/midlevel-reps/tree/visualpr...  \n",
       "17  github.com/alexsax/midlevel-reps/tree/visualpr...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taskonomy_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### CLIP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_dictlist = [\n",
    "    \n",
    "    {'model': 'RN50', 'model_display_name': 'CLiP-ResNet50', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'RN101', 'model_display_name': 'CLiP-ResNet101', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'RN50x4', 'model_display_name': 'CLiP-ResNet50x4', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'RN50x16', 'model_display_name': 'CLiP-ResNet50x16', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'RN50x64', 'model_display_name': 'CLiP-ResNet50x64', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'ViT-B/32', 'model_display_name': 'CLiP-ViT-B/32', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'ViT-B/16', 'model_display_name': 'CLiP-ViT-B/32', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'ViT-L/14', 'model_display_name': 'CLiP-ViT-L/14', 'model_class': 'Transformer'},\n",
    "    \n",
    "]\n",
    "    \n",
    "    \n",
    "for i, dict_i in enumerate(clip_dictlist):\n",
    "    dict_i = clip_dictlist[i]\n",
    "    dict_i['architecture'] = dict_i['model'].replace('RN','ResNet')\n",
    "    dict_i['description'] = '{}, a hybrid vision-language model.'.format(dict_i['model_display_name'])\n",
    "    \n",
    "clip_df = pd.DataFrame(clip_dictlist)\n",
    "clip_df['train_type'] = 'clip'\n",
    "clip_df['train_data'] = 'openai400M'\n",
    "clip_df['task_cluster'] = 'Vision-Language'\n",
    "clip_df['model_source'] = 'clip'\n",
    "clip_df['model_source_url'] = 'https://github.com/openai/CLIP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>model_class</th>\n",
       "      <th>architecture</th>\n",
       "      <th>description</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RN50</td>\n",
       "      <td>CLiP-ResNet50</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>ResNet50</td>\n",
       "      <td>CLiP-ResNet50, a hybrid vision-language model.</td>\n",
       "      <td>clip</td>\n",
       "      <td>openai400M</td>\n",
       "      <td>Vision-Language</td>\n",
       "      <td>clip</td>\n",
       "      <td>https://github.com/openai/CLIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ViT-L/14</td>\n",
       "      <td>CLiP-ViT-L/14</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>ViT-L/14</td>\n",
       "      <td>CLiP-ViT-L/14, a hybrid vision-language model.</td>\n",
       "      <td>clip</td>\n",
       "      <td>openai400M</td>\n",
       "      <td>Vision-Language</td>\n",
       "      <td>clip</td>\n",
       "      <td>https://github.com/openai/CLIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ViT-B/16</td>\n",
       "      <td>CLiP-ViT-B/32</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>ViT-B/16</td>\n",
       "      <td>CLiP-ViT-B/32, a hybrid vision-language model.</td>\n",
       "      <td>clip</td>\n",
       "      <td>openai400M</td>\n",
       "      <td>Vision-Language</td>\n",
       "      <td>clip</td>\n",
       "      <td>https://github.com/openai/CLIP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model model_display_name    model_class architecture  \\\n",
       "0      RN50      CLiP-ResNet50  Convolutional     ResNet50   \n",
       "7  ViT-L/14      CLiP-ViT-L/14    Transformer     ViT-L/14   \n",
       "6  ViT-B/16      CLiP-ViT-B/32    Transformer     ViT-B/16   \n",
       "\n",
       "                                      description train_type  train_data  \\\n",
       "0  CLiP-ResNet50, a hybrid vision-language model.       clip  openai400M   \n",
       "7  CLiP-ViT-L/14, a hybrid vision-language model.       clip  openai400M   \n",
       "6  CLiP-ViT-B/32, a hybrid vision-language model.       clip  openai400M   \n",
       "\n",
       "      task_cluster model_source                model_source_url  \n",
       "0  Vision-Language         clip  https://github.com/openai/CLIP  \n",
       "7  Vision-Language         clip  https://github.com/openai/CLIP  \n",
       "6  Vision-Language         clip  https://github.com/openai/CLIP  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### VISSL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vissl_dictlist = [\n",
    "    {'model': 'ResNet50-JigSaw-P100',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/jigsaw_rn50_in1k_ep105_perm2k_jigsaw_8gpu_resnet_17_07_20.db174a43/model_final_checkpoint_phase104.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-Colorization-Goyal19',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/converted_vissl_rn50_colorization_in1k_goyal19.torch'},\n",
    "    \n",
    "    {'model': 'ResNet50-JigSaw-Goyal19',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/converted_vissl_rn50_jigsaw_in1k_goyal19.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-RotNet',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/rotnet_rn50_in1k_ep105_rotnet_8gpu_resnet_17_07_20.46bada9f/model_final_checkpoint_phase125.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-ClusterFit-16K-RotNet',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/converted_vissl_rn50_rotnet_16kclusters_in1k_ep105.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-NPID',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/converted_vissl_rn50_npid_lemniscate_neg4k_stepLR_8gpu.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-PIRL',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/pirl_jigsaw_4node_pirl_jigsaw_4node_resnet_22_07_20.34377f59/model_final_checkpoint_phase799.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-SimCLR',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/simclr_rn50_1000ep_simclr_8node_resnet_16_07_20.afe428c7/model_final_checkpoint_phase999.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-DeepClusterV2-2x224',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/deepclusterv2_400ep_2x224_pretrain.pth.tar'},\n",
    " \n",
    "    {'model': 'ResNet50-DeepClusterV2-2x224+6x96',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/deepclusterv2_800ep_pretrain.pth.tar'},\n",
    " \n",
    "    {'model': 'ResNet50-SwAV-BS4096-2x224',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/swav_in1k_rn50_400ep_swav_8node_resnet_27_07_20.a5990fc9/model_final_checkpoint_phase399.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-SwAV-BS4096-2x224+6x96',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/swav_8node_2x224_rn50_in1k_swav_8node_resnet_30_07_20.c8fd7169/model_final_checkpoint_phase399.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-MoCoV2-BS256',\n",
    "  'weights_url': 'https://dl.fbaipublicfiles.com/vissl/model_zoo/moco_v2_1node_lr.03_step_b32_zero_init/model_final_checkpoint_phase199.torch'},\n",
    " \n",
    "    {'model': 'ResNet50-BarlowTwins-BS2048',\n",
    "  'weights_url': ' https://dl.fbaipublicfiles.com/vissl/model_zoo/barlow_twins/barlow_twins_32gpus_4node_imagenet1k_1000ep_resnet50.torch'}\n",
    "]\n",
    "\n",
    "for i, dict_i in enumerate(vissl_dictlist):\n",
    "    dict_i = vissl_dictlist[i]\n",
    "    dict_i['model_display_name'] = dict_i['model']\n",
    "    dict_i['description'] = '{}, a self-supervised representation learner trained on ImageNet.'.format(dict_i['model'])\n",
    "    \n",
    "vissl_df = pd.DataFrame(vissl_dictlist)\n",
    "vissl_df['architecture'] = 'ResNet50'\n",
    "vissl_df['train_type'] = 'selfsupervised'\n",
    "vissl_df['train_data'] = 'imagenet'\n",
    "vissl_df['task_cluster'] = 'Self-Supervised'\n",
    "vissl_df['model_source'] = 'vissl'\n",
    "vissl_df['model_class'] = 'Convolutional'\n",
    "vissl_df['model_source_url'] = 'https://github.com/facebookresearch/vissl/'\n",
    "\n",
    "vissl_df['model_display_name'] = vissl_df['model_display_name'].apply(lambda x: x.replace('-2x224', ''))\n",
    "vissl_df['model_display_name'] = vissl_df['model_display_name'].apply(lambda x: x.replace('+6x96', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>weights_url</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>description</th>\n",
       "      <th>architecture</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ResNet50-DeepClusterV2-2x224</td>\n",
       "      <td>https://dl.fbaipublicfiles.com/vissl/model_zoo...</td>\n",
       "      <td>ResNet50-DeepClusterV2</td>\n",
       "      <td>ResNet50-DeepClusterV2-2x224, a self-supervise...</td>\n",
       "      <td>ResNet50</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Self-Supervised</td>\n",
       "      <td>vissl</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ResNet50-PIRL</td>\n",
       "      <td>https://dl.fbaipublicfiles.com/vissl/model_zoo...</td>\n",
       "      <td>ResNet50-PIRL</td>\n",
       "      <td>ResNet50-PIRL, a self-supervised representatio...</td>\n",
       "      <td>ResNet50</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Self-Supervised</td>\n",
       "      <td>vissl</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ResNet50-BarlowTwins-BS2048</td>\n",
       "      <td>https://dl.fbaipublicfiles.com/vissl/model_zo...</td>\n",
       "      <td>ResNet50-BarlowTwins-BS2048</td>\n",
       "      <td>ResNet50-BarlowTwins-BS2048, a self-supervised...</td>\n",
       "      <td>ResNet50</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Self-Supervised</td>\n",
       "      <td>vissl</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model  \\\n",
       "8   ResNet50-DeepClusterV2-2x224   \n",
       "6                  ResNet50-PIRL   \n",
       "13   ResNet50-BarlowTwins-BS2048   \n",
       "\n",
       "                                          weights_url  \\\n",
       "8   https://dl.fbaipublicfiles.com/vissl/model_zoo...   \n",
       "6   https://dl.fbaipublicfiles.com/vissl/model_zoo...   \n",
       "13   https://dl.fbaipublicfiles.com/vissl/model_zo...   \n",
       "\n",
       "             model_display_name  \\\n",
       "8        ResNet50-DeepClusterV2   \n",
       "6                 ResNet50-PIRL   \n",
       "13  ResNet50-BarlowTwins-BS2048   \n",
       "\n",
       "                                          description architecture  \\\n",
       "8   ResNet50-DeepClusterV2-2x224, a self-supervise...     ResNet50   \n",
       "6   ResNet50-PIRL, a self-supervised representatio...     ResNet50   \n",
       "13  ResNet50-BarlowTwins-BS2048, a self-supervised...     ResNet50   \n",
       "\n",
       "        train_type train_data     task_cluster model_source    model_class  \\\n",
       "8   selfsupervised   imagenet  Self-Supervised        vissl  Convolutional   \n",
       "6   selfsupervised   imagenet  Self-Supervised        vissl  Convolutional   \n",
       "13  selfsupervised   imagenet  Self-Supervised        vissl  Convolutional   \n",
       "\n",
       "                              model_source_url  \n",
       "8   https://github.com/facebookresearch/vissl/  \n",
       "6   https://github.com/facebookresearch/vissl/  \n",
       "13  https://github.com/facebookresearch/vissl/  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vissl_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Dino Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_models = ['dino_vits16',\n",
    "               'dino_vits8',\n",
    "               'dino_vitb16',\n",
    "               'dino_vitb8',\n",
    "               'dino_xcit_small_12_p16',\n",
    "               'dino_xcit_small_12_p8',\n",
    "               'dino_xcit_medium_24_p16',\n",
    "               'dino_xcit_medium_24_p8',\n",
    "               'dino_resnet50']\n",
    "\n",
    "\n",
    "dino_dictlist = [\n",
    "    \n",
    "    {'model': 'dino_vits16', 'model_display_name': 'Dino-VIT-S16'},\n",
    "    \n",
    "    {'model': 'dino_vits8', 'model_display_name': 'Dino-VIT-S8'},\n",
    "    \n",
    "    {'model': 'dino_vitb16', 'model_display_name': 'Dino-VIT-B16'},\n",
    "    \n",
    "    {'model': 'dino_vitb8', 'model_display_name': 'Dino-VIT-B8'},\n",
    "    \n",
    "    {'model': 'dino_xcit_small_12_p16', 'model_display_name': 'Dino-XCIT-S12-P16'},\n",
    "    \n",
    "    {'model': 'dino_xcit_small_12_p8', 'model_display_name': 'Dino-XCIT-S12-P8'},\n",
    "    \n",
    "    {'model': 'dino_xcit_medium_24_p16', 'model_display_name': 'Dino-XCIT-M24-P16'},\n",
    "    \n",
    "    {'model': 'dino_xcit_medium_24_p8', 'model_display_name': 'Dino-XCIT-M24-P8'},\n",
    "    \n",
    "    {'model': 'dino_resnet50', 'model_display_name': 'Dino-ResNet50'}\n",
    "    \n",
    "]\n",
    "\n",
    "train_type_text = {\n",
    "    'selfsupervised': 'self supervision',\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "}   \n",
    "    \n",
    "for i, dict_i in enumerate(dino_dictlist):\n",
    "    dict_i = dino_dictlist[i]\n",
    "    dict_i['train_type'] = 'selfsupervised'\n",
    "    model_class = 'Transformer'\n",
    "    if 'resnet50' in dict_i['model']:\n",
    "        model_class = 'Convolutional'\n",
    "    dict_i['architecture'] = '_'.join(dict_i['model'].split('_')[1:])\n",
    "    dict_i['model_class'] = model_class\n",
    "    dict_i['train_data'] = 'imagenet'\n",
    "    dict_i['description'] = '{} trained via {} with the {} dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                           train_type_text[dict_i['train_type']],\n",
    "                                                                           train_data_text[dict_i['train_data']])\n",
    "    \n",
    "dino_df = pd.DataFrame(dino_dictlist)\n",
    "dino_df['task_cluster'] = 'SelfSupervised'\n",
    "dino_df['model_source'] = 'dino'\n",
    "dino_df['model_source_url'] = 'https://github.com/facebookresearch/dino'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>train_type</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_class</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dino_vits8</td>\n",
       "      <td>Dino-VIT-S8</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>vits8</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Dino-VIT-S8 trained via self supervision with ...</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>dino</td>\n",
       "      <td>https://github.com/facebookresearch/dino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dino_xcit_small_12_p16</td>\n",
       "      <td>Dino-XCIT-S12-P16</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>xcit_small_12_p16</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Dino-XCIT-S12-P16 trained via self supervision...</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>dino</td>\n",
       "      <td>https://github.com/facebookresearch/dino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dino_vitb8</td>\n",
       "      <td>Dino-VIT-B8</td>\n",
       "      <td>selfsupervised</td>\n",
       "      <td>vitb8</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Dino-VIT-B8 trained via self supervision with ...</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>dino</td>\n",
       "      <td>https://github.com/facebookresearch/dino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model model_display_name      train_type  \\\n",
       "1              dino_vits8        Dino-VIT-S8  selfsupervised   \n",
       "4  dino_xcit_small_12_p16  Dino-XCIT-S12-P16  selfsupervised   \n",
       "3              dino_vitb8        Dino-VIT-B8  selfsupervised   \n",
       "\n",
       "        architecture  model_class train_data  \\\n",
       "1              vits8  Transformer   imagenet   \n",
       "4  xcit_small_12_p16  Transformer   imagenet   \n",
       "3              vitb8  Transformer   imagenet   \n",
       "\n",
       "                                         description    task_cluster  \\\n",
       "1  Dino-VIT-S8 trained via self supervision with ...  SelfSupervised   \n",
       "4  Dino-XCIT-S12-P16 trained via self supervision...  SelfSupervised   \n",
       "3  Dino-VIT-B8 trained via self supervision with ...  SelfSupervised   \n",
       "\n",
       "  model_source                          model_source_url  \n",
       "1         dino  https://github.com/facebookresearch/dino  \n",
       "4         dino  https://github.com/facebookresearch/dino  \n",
       "3         dino  https://github.com/facebookresearch/dino  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### MiDas Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "midas_dictlist = [\n",
    "    {'model': 'DPT_Hybrid', 'model_display_name': 'DPT-Hybrid'},\n",
    "    {'model': 'DPT_Large', 'model_display_name': 'DPT-Large'},\n",
    "    {'model': 'MiDaS', 'model_display_name': 'MiDaS'},\n",
    "    {'model': 'MiDaS_small', 'model_display_name': 'MiDaS-S'} \n",
    "]\n",
    "\n",
    "for i, dict_i in enumerate(midas_dictlist):\n",
    "    dict_i = midas_dictlist[i]\n",
    "    dict_i['description'] = '{}, a monocular depth estimation model.'.format(dict_i['model_display_name'])\n",
    "    if 'DPT' in dict_i['model']:\n",
    "        dict_i['model_class'] = 'Transformer'\n",
    "    if 'MiDas' in dict_i['model']:\n",
    "        dict_i['model_class'] = 'Convolutional'\n",
    "    \n",
    "midas_df = pd.DataFrame(midas_dictlist)\n",
    "midas_df['architecture'] = midas_df['model']\n",
    "midas_df['train_type'] = 'monoculardepth'\n",
    "midas_df['train_data'] = 'ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HRWSI,ApolloScape,BlendedMVS,IRS'\n",
    "midas_df['task_cluster'] = 'MonocularDepth'\n",
    "midas_df['model_source'] = 'midas'\n",
    "midas_df['model_source_url'] = 'https://github.com/isl-org/MiDaS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>description</th>\n",
       "      <th>model_class</th>\n",
       "      <th>architecture</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DPT_Hybrid</td>\n",
       "      <td>DPT-Hybrid</td>\n",
       "      <td>DPT-Hybrid, a monocular depth estimation model.</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>DPT_Hybrid</td>\n",
       "      <td>monoculardepth</td>\n",
       "      <td>ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HR...</td>\n",
       "      <td>MonocularDepth</td>\n",
       "      <td>midas</td>\n",
       "      <td>https://github.com/isl-org/MiDaS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MiDaS_small</td>\n",
       "      <td>MiDaS-S</td>\n",
       "      <td>MiDaS-S, a monocular depth estimation model.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MiDaS_small</td>\n",
       "      <td>monoculardepth</td>\n",
       "      <td>ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HR...</td>\n",
       "      <td>MonocularDepth</td>\n",
       "      <td>midas</td>\n",
       "      <td>https://github.com/isl-org/MiDaS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DPT_Large</td>\n",
       "      <td>DPT-Large</td>\n",
       "      <td>DPT-Large, a monocular depth estimation model.</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>DPT_Large</td>\n",
       "      <td>monoculardepth</td>\n",
       "      <td>ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HR...</td>\n",
       "      <td>MonocularDepth</td>\n",
       "      <td>midas</td>\n",
       "      <td>https://github.com/isl-org/MiDaS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model model_display_name  \\\n",
       "0   DPT_Hybrid         DPT-Hybrid   \n",
       "3  MiDaS_small            MiDaS-S   \n",
       "1    DPT_Large          DPT-Large   \n",
       "\n",
       "                                       description  model_class architecture  \\\n",
       "0  DPT-Hybrid, a monocular depth estimation model.  Transformer   DPT_Hybrid   \n",
       "3     MiDaS-S, a monocular depth estimation model.          NaN  MiDaS_small   \n",
       "1   DPT-Large, a monocular depth estimation model.  Transformer    DPT_Large   \n",
       "\n",
       "       train_type                                         train_data  \\\n",
       "0  monoculardepth  ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HR...   \n",
       "3  monoculardepth  ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HR...   \n",
       "1  monoculardepth  ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HR...   \n",
       "\n",
       "     task_cluster model_source                  model_source_url  \n",
       "0  MonocularDepth        midas  https://github.com/isl-org/MiDaS  \n",
       "3  MonocularDepth        midas  https://github.com/isl-org/MiDaS  \n",
       "1  MonocularDepth        midas  https://github.com/isl-org/MiDaS  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midas_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### YoloV5 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_dictlist = [\n",
    "    {'model': 'yolov5l', 'model_display_name': 'YOLO-V5-L'},\n",
    "    {'model': 'yolov5m', 'model_display_name': 'YOLO-V5-M'},\n",
    "    {'model': 'yolov5n', 'model_display_name': 'YOLO-V5-N'},\n",
    "    {'model': 'yolov5s', 'model_display_name': 'YOLO-V5-S'},\n",
    "    {'model': 'yolov5x', 'model_display_name': 'YOLO-V5-S'},\n",
    "]\n",
    "\n",
    "for i, dict_i in enumerate(yolo_dictlist):\n",
    "    dict_i = yolo_dictlist[i]\n",
    "    dict_i['description'] = '{}, an object detection model.'.format(dict_i['model_display_name'])\n",
    "    \n",
    "yolo_df = pd.DataFrame(yolo_dictlist)\n",
    "yolo_df['architecture'] = yolo_df['model']\n",
    "yolo_df['model_class'] = 'Convolutional'\n",
    "yolo_df['train_type'] = 'yolo'\n",
    "yolo_df['train_data'] = 'coco,voc'\n",
    "yolo_df['task_cluster'] = 'Detection'\n",
    "yolo_df['model_source'] = 'yolo'\n",
    "yolo_df['model_source_url'] = 'https://github.com/ultralytics/yolov5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>description</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_class</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yolov5l</td>\n",
       "      <td>YOLO-V5-L</td>\n",
       "      <td>YOLO-V5-L, an object detection model.</td>\n",
       "      <td>yolov5l</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>yolo</td>\n",
       "      <td>coco,voc</td>\n",
       "      <td>Detection</td>\n",
       "      <td>yolo</td>\n",
       "      <td>https://github.com/ultralytics/yolov5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model model_display_name                            description  \\\n",
       "0  yolov5l          YOLO-V5-L  YOLO-V5-L, an object detection model.   \n",
       "\n",
       "  architecture    model_class train_type train_data task_cluster model_source  \\\n",
       "0      yolov5l  Convolutional       yolo   coco,voc    Detection         yolo   \n",
       "\n",
       "                        model_source_url  \n",
       "0  https://github.com/ultralytics/yolov5  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo_df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Detectron Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectron_dictlist = [\n",
    "    {'model': 'faster_rcnn_R_50_C4_3x', 'model_display_name': 'Faster-RCNN-ResNet50-C4',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_50_C4_3x.yaml'},\n",
    "        \n",
    "    {'model': 'faster_rcnn_R_50_DC5_3x', 'model_display_name': 'Faster-RCNN-ResNet50-DC5',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_50_DC5_3x.yaml'},\n",
    "    \n",
    "    {'model': 'faster_rcnn_R_50_FPN_3x', 'model_display_name': 'Faster-RCNN-ResNet50-FPN',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'faster_rcnn_R_101_C4_3x', 'model_display_name': 'Faster-RCNN-ResNet101-C4',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_101_C4_3x.yaml'},\n",
    "        \n",
    "    {'model': 'faster_rcnn_R_101_DC5_3x', 'model_display_name': 'Faster-RCNN-ResNet101-DC5',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_101_DC5_3x.yaml'},\n",
    "    \n",
    "    {'model': 'faster_rcnn_R_101_FPN_3x', 'model_display_name': 'Faster-RCNN-ResNet101-FPN',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'faster_rcnn_X_101_32x8d_FPN_3x', 'model_display_name': 'Faster-RCNN-X101-FPN',\n",
    "     'weights_url': 'COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'retinanet_R_50_FPN_3x', 'model_display_name': 'RetinaNet-ResNet50-FPN',\n",
    "     'weights_url': 'COCO-Detection/retinanet_R_50_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'retinanet_R_101_FPN_3x', 'model_display_name': 'RetinaNet-ResNet101-FPN',\n",
    "     'weights_url': 'COCO-Detection/retinanet_R_101_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_50_C4_3x', 'model_display_name': 'Mask-RCNN-ResNet50-C4',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_50_DC5_3x', 'model_display_name': 'Mask-RCNN-ResNet50-DC5',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_50_FPN_3x', 'model_display_name': 'Mask-RCNN-ResNet50-FPN',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_101_C4_3x', 'model_display_name': 'Mask-RCNN-ResNet101-C4',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_101_DC5_3x', 'model_display_name': 'Mask-RCNN-ResNet101-DC5',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_101_DC5_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_101_FPN_3x', 'model_display_name': 'Mask-RCNN-ResNet101-FPN',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_X_101_32x8d_FPN_3x', 'model_display_name': 'Mask-RCNN-X101-FPN',\n",
    "     'weights_url': 'COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml'},\n",
    "        \n",
    "    {'model': 'keypoint_rcnn_R_50_FPN_3x', 'model_display_name': 'Keypoint-RCNN-ResNet50-FPN',\n",
    "     'weights_url': 'COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml'},\n",
    "    \n",
    "    {'model': 'panoptic_fpn_R_50_3x', 'model_display_name': 'Panoptic-RCNN-ResNet50-FPN',\n",
    "     'weights_url': 'COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_50_FPN_1x', 'model_display_name': 'LVIS-RCNN-ResNet50-FPN',\n",
    "     'weights_url': 'LVISv0.5-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_R_101_FPN_1x', 'model_display_name': 'LVIS-RCNN-ResNet101-FPN',\n",
    "     'weights_url': 'LVISv0.5-InstanceSegmentation/mask_rcnn_R_101_FPN_1x.yaml'},\n",
    "    \n",
    "    {'model': 'mask_rcnn_X_101_32x8d_FPN_1x', 'model_display_name': 'LVIS-RCNN-X101-FPN',\n",
    "     'weights_url': 'LVISv0.5-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x.yaml'},\n",
    "    \n",
    "]\n",
    "\n",
    "for i, dict_i in enumerate(detectron_dictlist):\n",
    "    dict_i = detectron_dictlist[i]\n",
    "    train_data = 'coco2017'\n",
    "    if 'COCO-Detection' in dict_i['weights_url']:\n",
    "        train_type = 'detection'\n",
    "    if 'COCO-InstanceSegmentation' in dict_i['weights_url']:\n",
    "        train_type = 'segmentation'\n",
    "    if 'new_baselines' in dict_i['weights_url']:\n",
    "        train_type = 'segmentation'\n",
    "    if 'CoCo-Keypoints' in dict_i['weights_url']:\n",
    "        train_type = 'keypoints'\n",
    "    if 'COCO-PanopticSegmentation' in dict_i['weights_url']:\n",
    "        train_type = 'panoptics'\n",
    "    if 'COCO-PanopticSegmentation' in dict_i['weights_url']:\n",
    "        train_type = 'panoptics'\n",
    "    if 'LVISv0.5-InstanceSegmentation' in dict_i['weights_url']:\n",
    "        train_type = 'segmentation'\n",
    "        train_data = 'LVIS'\n",
    "    dict_i['train_type'] = train_type\n",
    "    dict_i['train_data'] = train_data\n",
    "    dict_i['description'] = '{}, trained on {} with the CoCo2017 dataset.'.format(dict_i['model_display_name'], train_type)\n",
    "    \n",
    "detectron_df = pd.DataFrame(detectron_dictlist)\n",
    "detectron_df['architecture'] = detectron_df['model']\n",
    "detectron_df['task_cluster'] = 'Detection|Segmentation'\n",
    "detectron_df['model_class'] = 'Convolutional'\n",
    "detectron_df['model_source'] = 'detectron'\n",
    "detectron_df['model_source_url'] = 'https://github.com/facebookresearch/detectron2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>weights_url</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>architecture</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mask_rcnn_R_101_C4_3x</td>\n",
       "      <td>Mask-RCNN-ResNet101-C4</td>\n",
       "      <td>COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3...</td>\n",
       "      <td>segmentation</td>\n",
       "      <td>coco2017</td>\n",
       "      <td>Mask-RCNN-ResNet101-C4, trained on segmentatio...</td>\n",
       "      <td>mask_rcnn_R_101_C4_3x</td>\n",
       "      <td>Detection|Segmentation</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>detectron</td>\n",
       "      <td>https://github.com/facebookresearch/detectron2/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mask_rcnn_R_50_FPN_3x</td>\n",
       "      <td>Mask-RCNN-ResNet50-FPN</td>\n",
       "      <td>COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3...</td>\n",
       "      <td>segmentation</td>\n",
       "      <td>coco2017</td>\n",
       "      <td>Mask-RCNN-ResNet50-FPN, trained on segmentatio...</td>\n",
       "      <td>mask_rcnn_R_50_FPN_3x</td>\n",
       "      <td>Detection|Segmentation</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>detectron</td>\n",
       "      <td>https://github.com/facebookresearch/detectron2/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faster_rcnn_R_101_DC5_3x</td>\n",
       "      <td>Faster-RCNN-ResNet101-DC5</td>\n",
       "      <td>COCO-Detection/faster_rcnn_R_101_DC5_3x.yaml</td>\n",
       "      <td>detection</td>\n",
       "      <td>coco2017</td>\n",
       "      <td>Faster-RCNN-ResNet101-DC5, trained on detectio...</td>\n",
       "      <td>faster_rcnn_R_101_DC5_3x</td>\n",
       "      <td>Detection|Segmentation</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>detectron</td>\n",
       "      <td>https://github.com/facebookresearch/detectron2/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model         model_display_name  \\\n",
       "12     mask_rcnn_R_101_C4_3x     Mask-RCNN-ResNet101-C4   \n",
       "11     mask_rcnn_R_50_FPN_3x     Mask-RCNN-ResNet50-FPN   \n",
       "4   faster_rcnn_R_101_DC5_3x  Faster-RCNN-ResNet101-DC5   \n",
       "\n",
       "                                          weights_url    train_type  \\\n",
       "12  COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3...  segmentation   \n",
       "11  COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3...  segmentation   \n",
       "4        COCO-Detection/faster_rcnn_R_101_DC5_3x.yaml     detection   \n",
       "\n",
       "   train_data                                        description  \\\n",
       "12   coco2017  Mask-RCNN-ResNet101-C4, trained on segmentatio...   \n",
       "11   coco2017  Mask-RCNN-ResNet50-FPN, trained on segmentatio...   \n",
       "4    coco2017  Faster-RCNN-ResNet101-DC5, trained on detectio...   \n",
       "\n",
       "                architecture            task_cluster    model_class  \\\n",
       "12     mask_rcnn_R_101_C4_3x  Detection|Segmentation  Convolutional   \n",
       "11     mask_rcnn_R_50_FPN_3x  Detection|Segmentation  Convolutional   \n",
       "4   faster_rcnn_R_101_DC5_3x  Detection|Segmentation  Convolutional   \n",
       "\n",
       "   model_source                                 model_source_url  \n",
       "12    detectron  https://github.com/facebookresearch/detectron2/  \n",
       "11    detectron  https://github.com/facebookresearch/detectron2/  \n",
       "4     detectron  https://github.com/facebookresearch/detectron2/  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detectron_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Timm (+Pretrain) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_pluspretrain_dictlist = [\n",
    "    \n",
    "    {'model': 'adv_inception_v3', 'model_display_name': 'Adversarial-Inception-V3', \n",
    "     'architecture': 'inception_v3',\n",
    "     'train_type': 'adversarial', 'train_data': 'imagenet', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'beit_base_patch16_224', 'model_display_name': 'BEiT-S-P16',\n",
    "     'architecture': 'beit_base_patch16_224',\n",
    "     'train_type': 'bert_pretraining', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'beit_large_patch16_224', 'model_display_name': 'BEiT-L-P16',\n",
    "     'architecture': 'beit_large_patch16_224',\n",
    "     'train_type': 'bert_pretraining', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'resnetv2_50x1_bitm', 'model_display_name': 'ResNetV2-50x1-BitM', \n",
    "     'architecture': 'resnetv2_50x1',\n",
    "     'train_type': 'big_transfer', 'train_data': 'big_transfer', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'resnetv2_101x3_bitm', 'model_display_name': 'ResNetV2-101x3-BitM',\n",
    "     'architecture': 'resnetv2_101x3',\n",
    "     'train_type': 'big_transfer', 'train_data': 'big_transfer', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'resnetv2_152x4_bitm', 'model_display_name': 'ResNetV2-152x4-BitM',\n",
    "     'architecture': 'resnetv2_152x4',\n",
    "     'train_type': 'big_transfer', 'train_data': 'big_transfer', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'resnetv2_50x1_bitm_in21k', 'model_display_name': 'ResNetV2-50x1-BitM-IN21K', \n",
    "     'architecture': 'resnetv2_50x1',\n",
    "     'train_type': 'big_transfer', 'train_data': 'big_transfer', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'resnetv2_101x3_bitm_in21k', 'model_display_name': 'ResNetV2-101x3-BitM-IN21K', \n",
    "     'architecture': 'resnetv2_101x3',\n",
    "     'train_type': 'big_transfer', 'train_data': 'big_transfer', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'resnetv2_152x4_bitm_in21k', 'model_display_name': 'ResNetV2-152x4-BitM-IN21K', \n",
    "     'architecture': 'resnetv2_152x4',\n",
    "     'train_type': 'big_transfer', 'train_data': 'big_transfer', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'convnext_base_in22k', 'model_display_name': 'ConvNext-Base-IN21K', \n",
    "     'architecture': 'convnext_base',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'convnext_large_in22k', 'model_display_name': 'ConvNext-Large-IN21K', \n",
    "     'architecture': 'convnext_large',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Convolutional'},\n",
    "\n",
    "    {'model': 'mixer_b16_224_in21k', 'model_display_name': 'Mixer-B16-IN22K',\n",
    "     'architecture': 'mixer_b16_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'mixer_l16_224_in21k', 'model_display_name': 'Mixer-L16-IN22K',\n",
    "     'architecture': 'mixer_l16_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'mobilenetv3_large_100_miil_in21k', 'model_display_name': 'MobileNet-V3-Large-IN21K', \n",
    "     'architecture': 'mobilenetv3_large_100',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'swin_base_patch4_window7_224_in22k', 'model_display_name': 'Swin-B-P4-W7-IN21K', \n",
    "     'architecture': 'swin_base_patch4_window7_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'swin_large_patch4_window7_224_in22k', 'model_display_name': 'Swin-L-P4-W7-IN21K', \n",
    "     'architecture': 'swin_large_patch4_window7_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'resmlp_big_24_224_in22ft1k', 'model_display_name': 'ResMLP-Big-24-IN21K', \n",
    "     'architecture': 'resmlp_big_24_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'MLP-Mixer'},\n",
    "    \n",
    "    {'model': 'vit_base_r50_s16_224_in21k', 'model_display_name': 'ViT-B-R50-S16-IN21K', \n",
    "     'architecture': 'vit_base_r50_s16_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_base_patch16_224_in21k', 'model_display_name': 'ViT-B-P16-IN21K', \n",
    "     'architecture': 'vit_base_patch16_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_small_patch16_224_in21k', 'model_display_name': 'ViT-S-P16-IN21K', \n",
    "     'architecture': 'vit_small_patch16_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_large_patch16_224_in21k', 'model_display_name': 'ViT-L-P16-IN21K', \n",
    "     'architecture': 'vit_large_patch16_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_base_patch32_224_in21k', 'model_display_name': 'ViT-B-P32-IN21K', \n",
    "     'architecture': 'vit_base_patch32_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_small_patch32_224_in21k', 'model_display_name': 'ViT-S-P32-IN21K', \n",
    "     'architecture': 'vit_small_patch32_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_large_patch32_224_in21k', 'model_display_name': 'ViT-L-P32-IN21K', \n",
    "     'architecture': 'vit_large_patch32_224',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'ssl_resnet18', 'model_display_name': 'ResNet18-SSL',\n",
    "     'architecture': 'resnet18',\n",
    "     'train_type': 'semi-supervised', 'train_data': 'YFCC100M', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'ssl_resnet50', 'model_display_name': 'ResNet50-SSL',\n",
    "     'architecture': 'resnet50',\n",
    "     'train_type': 'semi-supervised', 'train_data': 'YFCC100M', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'ssl_resnext101_32x4d', 'model_display_name': 'ResNext101-32x4D-SSL', \n",
    "     'architecture': 'resnext101_32x4d',\n",
    "     'train_type': 'semi-supervised', 'train_data': 'YFCC100M', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'swsl_resnet18', 'model_display_name': 'Resnet18-SWSL', \n",
    "     'architecture': 'resnet18',\n",
    "     'train_type': 'semi-weakly-supervised', 'train_data': 'Instagram', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'swsl_resnet50', 'model_display_name': 'Resnet50-SWSL',\n",
    "     'architecture': 'resnet50',\n",
    "     'train_type': 'semi-weakly-supervised', 'train_data': 'Instagram', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'swsl_resnext101_32x4d', 'model_display_name': 'ResNext101-32x4D-SWSL',\n",
    "     'architecture': 'resnext101_32x4d',\n",
    "     'train_type': 'semi-weakly-supervised', 'train_data': 'Instagram', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'vit_base_patch16_sam_224', 'model_display_name': 'ViT-B-P16-SAM', \n",
    "     'architecture': 'vit_base_patch16_224',\n",
    "     'train_type': 'sam_pretraining', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_small_patch32_sam_224', 'model_display_name': 'ViT-S-P32-SAM', \n",
    "     'architecture': 'vit_base_patch16_224',\n",
    "     'train_type': 'sam_pretraining', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "\n",
    "]\n",
    "\n",
    "tf_additions = []\n",
    "for depth in range(8):\n",
    "    for train_type_index in range(3):\n",
    "        train_type_codex = ['','_ap','_ns'][train_type_index]\n",
    "        train_type_title = ['','AP','NS'][train_type_index]\n",
    "        train_type_label = ['classification','adversarial','noisy_student'][train_type_index]\n",
    "        tf_additions.append({'model': 'tf_efficientnet_b{}{}'.format(depth, train_type_codex),\n",
    "                             'architecture': 'tf_efficientnet_b{}'.format(depth),\n",
    "                             'model_display_name': 'TF-EfficientNet-B{}{}'.format(depth, train_type_title),\n",
    "                             'train_type': train_type_label, 'train_data': 'imagenet', 'model_class': 'Convolutional'})\n",
    "        \n",
    "timm_pluspretrain_dictlist += tf_additions\n",
    "\n",
    "train_type_text = {\n",
    "    'classification': 'image classification',\n",
    "    'bert_pretraining': 'masked-input-pretrained classification',\n",
    "    'adversarial': 'adversarial classification',\n",
    "    'noisy_student': 'noisy student classification',\n",
    "    'big_transfer': 'large dataset pretraining',\n",
    "    'sam_pretraining': 'sharpness-aware pretraining',\n",
    "    'semi-supervised': 'semi-supervised image classification',\n",
    "    'semi-weakly-supervised': 'semi-weakly-supervised image classification',\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "    'imagenet21k': 'ImageNet21K',\n",
    "    'big_transfer': 'BiT-M',\n",
    "    'YFCC100M': 'YFCC100M',\n",
    "    'Instagram': 'Instagram', \n",
    "}\n",
    "\n",
    "for i, dict_i in enumerate(timm_pluspretrain_dictlist):\n",
    "    dict_i = timm_pluspretrain_dictlist[i]\n",
    "    dict_i['description'] = '{} trained on {} with the {} dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                           train_type_text[dict_i['train_type']],\n",
    "                                                                           train_data_text[dict_i['train_data']])\n",
    "        \n",
    "timm_pluspretrain_df = pd.DataFrame(timm_pluspretrain_dictlist)\n",
    "timm_pluspretrain_df['task_cluster'] = 'Semantic'\n",
    "timm_pluspretrain_df['model_source'] = 'timm'\n",
    "timm_pluspretrain_df['model_source_url'] = 'https://github.com/rwightman/pytorch-image-models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>architecture</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>model_class</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>tf_efficientnet_b7</td>\n",
       "      <td>TF-EfficientNet-B7</td>\n",
       "      <td>tf_efficientnet_b7</td>\n",
       "      <td>classification</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>TF-EfficientNet-B7 trained on image classifica...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>convnext_large_in22k</td>\n",
       "      <td>ConvNext-Large-IN21K</td>\n",
       "      <td>convnext_large</td>\n",
       "      <td>classification</td>\n",
       "      <td>imagenet21k</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>ConvNext-Large-IN21K trained on image classifi...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>tf_efficientnet_b4_ap</td>\n",
       "      <td>TF-EfficientNet-B4AP</td>\n",
       "      <td>tf_efficientnet_b4</td>\n",
       "      <td>adversarial</td>\n",
       "      <td>imagenet</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>TF-EfficientNet-B4AP trained on adversarial cl...</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>timm</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-mod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model    model_display_name        architecture  \\\n",
       "53     tf_efficientnet_b7    TF-EfficientNet-B7  tf_efficientnet_b7   \n",
       "10   convnext_large_in22k  ConvNext-Large-IN21K      convnext_large   \n",
       "45  tf_efficientnet_b4_ap  TF-EfficientNet-B4AP  tf_efficientnet_b4   \n",
       "\n",
       "        train_type   train_data    model_class  \\\n",
       "53  classification     imagenet  Convolutional   \n",
       "10  classification  imagenet21k  Convolutional   \n",
       "45     adversarial     imagenet  Convolutional   \n",
       "\n",
       "                                          description task_cluster  \\\n",
       "53  TF-EfficientNet-B7 trained on image classifica...     Semantic   \n",
       "10  ConvNext-Large-IN21K trained on image classifi...     Semantic   \n",
       "45  TF-EfficientNet-B4AP trained on adversarial cl...     Semantic   \n",
       "\n",
       "   model_source                                   model_source_url  \n",
       "53         timm  https://github.com/rwightman/pytorch-image-mod...  \n",
       "10         timm  https://github.com/rwightman/pytorch-image-mod...  \n",
       "45         timm  https://github.com/rwightman/pytorch-image-mod...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm_pluspretrain_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Timm (+Resolution) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_plusres_dictlist = [\n",
    "    \n",
    "    {'model': 'beit_base_patch16_384', 'model_display_name': 'BEiT-B-P16',\n",
    "     'train_type': 'bert_pretraining', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'beit_base_patch16_384', 'model_display_name': 'BEiT-L-P16',\n",
    "     'train_type': 'bert_pretraining', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'beit_base_patch16_512', 'model_display_name': 'BEiT-L-P16',\n",
    "     'train_type': 'bert_pretraining', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'cait_s24_384', 'model_display_name': 'CaiT-S-24',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'cait_s24_384', 'model_display_name': 'CaiT-S-24',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'convnext_base_384_in22ft1k', 'model_display_name': 'ConvNext-B',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'convnext_large_384_in22ft1k', 'model_display_name': 'ConvNext-L',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Convolutional'},\n",
    "    \n",
    "    {'model': 'crossvit_15_dagger_408', 'model_display_name': 'CrossViT-18-Dagger',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'deit_base_patch16_384', 'model_display_name': 'Deit-B-P16',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'levit_384', 'model_display_name': 'LeViT',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'swin_base_patch4_window12_384', 'model_display_name': 'Swin-B-P4-W12',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'swin_large_patch4_window12_384', 'model_display_name': 'Swin-L-P4-W12',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'swin_large_patch4_window12_384', 'model_display_name': 'Swin-L-P4-W12',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet21k', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_base_patch16_384', 'model_display_name': 'ViT-B-P16',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_base_patch32_384', 'model_display_name': 'ViT-B-P32',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_large_patch16_384', 'model_display_name': 'ViT-L-P16',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_large_patch32_384', 'model_display_name': 'ViT-L-P32',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_small_patch16_384', 'model_display_name': 'ViT-S-P16',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'vit_small_patch32_384', 'model_display_name': 'ViT-S-P32',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "    {'model': 'volo_d1_384', 'model_display_name': 'VoLo-D1',\n",
    "     'train_type': 'classification', 'train_data': 'imagenet', 'model_class': 'Transformer'},\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SLIP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "slip_dictlist = [\n",
    "    \n",
    "    {'model': 'ViT-S-SimCLR', 'train_type': 'SimCLR'},\n",
    "    {'model': 'ViT-S-CLIP', 'train_type': 'CLIP'},\n",
    "    {'model': 'ViT-S-SLIP', 'train_type': 'SLIP'},\n",
    "    {'model': 'ViT-B-SimCLR', 'train_type': 'SimCLR'},\n",
    "    {'model': 'ViT-B-CLIP', 'train_type': 'CLIP'},\n",
    "    {'model': 'ViT-B-SLIP', 'train_type': 'SLIP'},\n",
    "    {'model': 'ViT-L-SimCLR', 'train_type': 'SimCLR'},\n",
    "    {'model': 'ViT-L-CLIP', 'train_type': 'CLIP'},\n",
    "    {'model': 'ViT-L-SLIP', 'train_type': 'SLIP'},\n",
    "    {'model': 'ViT-S-SLIP-Ep100', 'train_type': 'SLIP'},\n",
    "    {'model': 'ViT-B-SLIP-Ep100', 'train_type': 'SLIP'},\n",
    "    {'model': 'ViT-L-SLIP-Ep100', 'train_type': 'SLIP'},\n",
    "    {'model': 'ViT-L-CLIP-CC12M', 'train_type': 'CLIP'},\n",
    "    {'model': 'ViT-L-SLIP-CC12M', 'train_type': 'SLIP'},  \n",
    "    \n",
    "]\n",
    "\n",
    "train_type_text = {\n",
    "    'SimCLR': 'pure self-supervision',\n",
    "    'CLIP': 'pure language supervision',\n",
    "    'SLIP': 'combined self- and language supervision',\n",
    "}\n",
    "    \n",
    "for i, dict_i in enumerate(slip_dictlist):\n",
    "    dict_i = slip_dictlist[i]\n",
    "    dict_i['architecture'] = '-'.join(dict_i['model'].split('-')[:2])\n",
    "    dict_i['model_display_name'] = dict_i['model']\n",
    "    dict_i['model_class'] = 'Transformer'\n",
    "    if 'SimCLR' in dict_i['model']:\n",
    "        dict_i['task_cluster'] = 'SelfSupervised'\n",
    "    if 'CLIP' in dict_i['model']:\n",
    "        dict_i['task_cluster'] = 'Vision-Language'\n",
    "    if 'SLIP' in dict_i['model']:\n",
    "        dict_i['task_cluster'] = 'Vision-Language'\n",
    "    dict_i['train_data'] = 'YFCC15M'\n",
    "    dict_i['description'] = '{} trained via {} with the YFCC15M dataset.'.format(dict_i['model_display_name'],\n",
    "                                                                           train_type_text[dict_i['train_type']])\n",
    "    dict_i['train_type'] = 'slip'\n",
    "    \n",
    "slip_df = pd.DataFrame(slip_dictlist)\n",
    "slip_df['model_source'] = 'slip'\n",
    "slip_df['model_source_url'] = 'https://github.com/facebookresearch/slip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_type</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>model_class</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>train_data</th>\n",
       "      <th>description</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ViT-B-SLIP</td>\n",
       "      <td>slip</td>\n",
       "      <td>ViT-B</td>\n",
       "      <td>ViT-B-SLIP</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>Vision-Language</td>\n",
       "      <td>YFCC15M</td>\n",
       "      <td>ViT-B-SLIP trained via combined self- and lang...</td>\n",
       "      <td>slip</td>\n",
       "      <td>https://github.com/facebookresearch/slip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ViT-S-SLIP</td>\n",
       "      <td>slip</td>\n",
       "      <td>ViT-S</td>\n",
       "      <td>ViT-S-SLIP</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>Vision-Language</td>\n",
       "      <td>YFCC15M</td>\n",
       "      <td>ViT-S-SLIP trained via combined self- and lang...</td>\n",
       "      <td>slip</td>\n",
       "      <td>https://github.com/facebookresearch/slip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ViT-L-SimCLR</td>\n",
       "      <td>slip</td>\n",
       "      <td>ViT-L</td>\n",
       "      <td>ViT-L-SimCLR</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>YFCC15M</td>\n",
       "      <td>ViT-L-SimCLR trained via pure self-supervision...</td>\n",
       "      <td>slip</td>\n",
       "      <td>https://github.com/facebookresearch/slip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model train_type architecture model_display_name  model_class  \\\n",
       "5    ViT-B-SLIP       slip        ViT-B         ViT-B-SLIP  Transformer   \n",
       "2    ViT-S-SLIP       slip        ViT-S         ViT-S-SLIP  Transformer   \n",
       "6  ViT-L-SimCLR       slip        ViT-L       ViT-L-SimCLR  Transformer   \n",
       "\n",
       "      task_cluster train_data  \\\n",
       "5  Vision-Language    YFCC15M   \n",
       "2  Vision-Language    YFCC15M   \n",
       "6   SelfSupervised    YFCC15M   \n",
       "\n",
       "                                         description model_source  \\\n",
       "5  ViT-B-SLIP trained via combined self- and lang...         slip   \n",
       "2  ViT-S-SLIP trained via combined self- and lang...         slip   \n",
       "6  ViT-L-SimCLR trained via pure self-supervision...         slip   \n",
       "\n",
       "                           model_source_url  \n",
       "5  https://github.com/facebookresearch/slip  \n",
       "2  https://github.com/facebookresearch/slip  \n",
       "6  https://github.com/facebookresearch/slip  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slip_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SEER Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seer_model_list = [\n",
    "    'RegNet-32Gf-SEER',\n",
    "    'RegNet-32Gf-SEER-INFT',\n",
    "    'RegNet-64Gf-SEER',\n",
    "    'RegNet-64Gf-SEER-INFT',\n",
    "    'RegNet-128Gf-SEER',\n",
    "    'RegNet-128Gf-SEER-INFT',\n",
    "    'RegNet-256Gf-SEER',\n",
    "    'RegNet-256Gf-SEER-INFT',\n",
    "]\n",
    "\n",
    "seer_dictlist = []\n",
    "for model in seer_model_list:\n",
    "    dict_i = {'model': model, 'model_display_name': model, 'train_type': 'seer', 'train_data': 'random1B'}\n",
    "    dict_i['architecture'] = '-'.join(model.split('-')[:2])\n",
    "    dict_i['model_class'] = 'Convolutional'\n",
    "    dict_i['description'] = ('{} trained via large-scale self-supervision on 1 billion images.'\n",
    "                             .format(dict_i['architecture']))\n",
    "                             \n",
    "    seer_dictlist.append(dict_i)\n",
    "\n",
    "seer_df = pd.DataFrame(seer_dictlist)\n",
    "seer_df['task_cluster'] = 'SelfSupervised'\n",
    "seer_df['model_source'] = 'seer'\n",
    "seer_df['model_source_url'] = 'https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md#seer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_class</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RegNet-256Gf-SEER-INFT</td>\n",
       "      <td>RegNet-256Gf-SEER-INFT</td>\n",
       "      <td>seer</td>\n",
       "      <td>random1B</td>\n",
       "      <td>RegNet-256Gf</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>RegNet-256Gf trained via large-scale self-supe...</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>seer</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/blob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RegNet-256Gf-SEER</td>\n",
       "      <td>RegNet-256Gf-SEER</td>\n",
       "      <td>seer</td>\n",
       "      <td>random1B</td>\n",
       "      <td>RegNet-256Gf</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>RegNet-256Gf trained via large-scale self-supe...</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>seer</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/blob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RegNet-32Gf-SEER</td>\n",
       "      <td>RegNet-32Gf-SEER</td>\n",
       "      <td>seer</td>\n",
       "      <td>random1B</td>\n",
       "      <td>RegNet-32Gf</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>RegNet-32Gf trained via large-scale self-super...</td>\n",
       "      <td>SelfSupervised</td>\n",
       "      <td>seer</td>\n",
       "      <td>https://github.com/facebookresearch/vissl/blob...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model      model_display_name train_type train_data  \\\n",
       "7  RegNet-256Gf-SEER-INFT  RegNet-256Gf-SEER-INFT       seer   random1B   \n",
       "6       RegNet-256Gf-SEER       RegNet-256Gf-SEER       seer   random1B   \n",
       "0        RegNet-32Gf-SEER        RegNet-32Gf-SEER       seer   random1B   \n",
       "\n",
       "   architecture    model_class  \\\n",
       "7  RegNet-256Gf  Convolutional   \n",
       "6  RegNet-256Gf  Convolutional   \n",
       "0   RegNet-32Gf  Convolutional   \n",
       "\n",
       "                                         description    task_cluster  \\\n",
       "7  RegNet-256Gf trained via large-scale self-supe...  SelfSupervised   \n",
       "6  RegNet-256Gf trained via large-scale self-supe...  SelfSupervised   \n",
       "0  RegNet-32Gf trained via large-scale self-super...  SelfSupervised   \n",
       "\n",
       "  model_source                                   model_source_url  \n",
       "7         seer  https://github.com/facebookresearch/vissl/blob...  \n",
       "6         seer  https://github.com/facebookresearch/vissl/blob...  \n",
       "0         seer  https://github.com/facebookresearch/vissl/blob...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seer_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### IPCL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipcl_options = {'alexnet_gn_ipcl_imagenet': 'ipcl1',\n",
    "                'alexnet_gn_ipcl_openimages': 'ipcl7',\n",
    "                'alexnet_gn_ipcl_places256': 'ipcl8',\n",
    "                'alexnet_gn_ipcl_vggface2': 'ipcl9',\n",
    "                'alexnet_gn_ipcl_random': 'ipcl16'}\n",
    "\n",
    "train_type_text = {\n",
    "    'ipcl': 'IPCL-Style-Self-Supervision',\n",
    "    'catsup': 'Category-Supervision',\n",
    "}\n",
    "\n",
    "train_data_text = {\n",
    "    'imagenet': 'ImageNet',\n",
    "    'openimages': 'OpenImages',\n",
    "    'places256': 'Places256',\n",
    "    'vggface2': 'VGGFace2',\n",
    "}\n",
    "\n",
    "ipcl_dictlist = []\n",
    "for ipcl_option in ipcl_options:\n",
    "    if 'random' not in ipcl_option:\n",
    "        dict_i = {'model': ipcl_option, 'train_type': ipcl_option.split('_')[-2], \n",
    "                  'train_data': ipcl_option.split('_')[-1]}\n",
    "        dict_i['model_display_name'] = ('AlexNet-GN-' + dict_i['train_type'].upper() \n",
    "                                        + train_data_text[dict_i['train_data']])\n",
    "        dict_i['description'] = ('AlexNet (modified with GroupNorm) trained via {} on {}.'\n",
    "                                 .format(train_type_text[dict_i['train_type']],\n",
    "                                         train_data_text[dict_i['train_data']]))\n",
    "        dict_i['TaskCluster'] = 'SelfSupervised' if dict_i['train_type'] == 'ipcl' else 'SelfSupervised'\n",
    "        \n",
    "        ipcl_dictlist.append(dict_i)\n",
    "        \n",
    "ipcl_df = pd.DataFrame(ipcl_dictlist)\n",
    "ipcl_df['architecture'] = 'alexnet_gn'\n",
    "ipcl_df['model_class'] = 'Convolutional'\n",
    "ipcl_df['model_source'] = 'open_ipcl'\n",
    "ipcl_df['model_source_url'] = 'https://github.com/harvard-visionlab/open_ipcl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### BIT Expert Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_expert_models = ['food','vehicle','instrument','flower','animal','object',\n",
    "                     'bird','mammal','arthropod','relation','abstraction']\n",
    "\n",
    "bit_expert_dictlist = []\n",
    "for expertise in bit_expert_models:\n",
    "    model = 'BiT-Expert-ResNet-V2-{}'.format(expertise.title())\n",
    "    dict_i = {'model': model, 'train_type': 'bit_expert', 'train_data': 'big_transfer'}\n",
    "    dict_i['architecture'] = 'ResNet50-V2'\n",
    "    dict_i['model_display_name'] = model\n",
    "    dict_i['model_class'] = 'Convolutional'\n",
    "    dict_i['description'] = ('{} trained first via ImageNet21K dataset, then as an expert on the {} subset.'\n",
    "                             .format(dict_i['architecture'], expertise.title()))\n",
    "    \n",
    "    bit_expert_dictlist.append(dict_i)\n",
    "    \n",
    "bit_expert_df = pd.DataFrame(bit_expert_dictlist)\n",
    "bit_expert_df['task_cluster'] = 'Expertise'\n",
    "bit_expert_df['model_source'] = 'bit_expert'\n",
    "bit_expert_df['model_source_url'] = 'https://tfhub.dev/google/collections/experts/bit/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>model_class</th>\n",
       "      <th>description</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BiT-Expert-ResNet-V2-Flower</td>\n",
       "      <td>bit_expert</td>\n",
       "      <td>big_transfer</td>\n",
       "      <td>ResNet50-V2</td>\n",
       "      <td>BiT-Expert-ResNet-V2-Flower</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>ResNet50-V2 trained first via ImageNet21K data...</td>\n",
       "      <td>Expertise</td>\n",
       "      <td>bit_expert</td>\n",
       "      <td>https://tfhub.dev/google/collections/experts/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BiT-Expert-ResNet-V2-Mammal</td>\n",
       "      <td>bit_expert</td>\n",
       "      <td>big_transfer</td>\n",
       "      <td>ResNet50-V2</td>\n",
       "      <td>BiT-Expert-ResNet-V2-Mammal</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>ResNet50-V2 trained first via ImageNet21K data...</td>\n",
       "      <td>Expertise</td>\n",
       "      <td>bit_expert</td>\n",
       "      <td>https://tfhub.dev/google/collections/experts/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiT-Expert-ResNet-V2-Vehicle</td>\n",
       "      <td>bit_expert</td>\n",
       "      <td>big_transfer</td>\n",
       "      <td>ResNet50-V2</td>\n",
       "      <td>BiT-Expert-ResNet-V2-Vehicle</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>ResNet50-V2 trained first via ImageNet21K data...</td>\n",
       "      <td>Expertise</td>\n",
       "      <td>bit_expert</td>\n",
       "      <td>https://tfhub.dev/google/collections/experts/b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  train_type    train_data architecture  \\\n",
       "3   BiT-Expert-ResNet-V2-Flower  bit_expert  big_transfer  ResNet50-V2   \n",
       "7   BiT-Expert-ResNet-V2-Mammal  bit_expert  big_transfer  ResNet50-V2   \n",
       "1  BiT-Expert-ResNet-V2-Vehicle  bit_expert  big_transfer  ResNet50-V2   \n",
       "\n",
       "             model_display_name    model_class  \\\n",
       "3   BiT-Expert-ResNet-V2-Flower  Convolutional   \n",
       "7   BiT-Expert-ResNet-V2-Mammal  Convolutional   \n",
       "1  BiT-Expert-ResNet-V2-Vehicle  Convolutional   \n",
       "\n",
       "                                         description task_cluster  \\\n",
       "3  ResNet50-V2 trained first via ImageNet21K data...    Expertise   \n",
       "7  ResNet50-V2 trained first via ImageNet21K data...    Expertise   \n",
       "1  ResNet50-V2 trained first via ImageNet21K data...    Expertise   \n",
       "\n",
       "  model_source                                   model_source_url  \n",
       "3   bit_expert  https://tfhub.dev/google/collections/experts/b...  \n",
       "7   bit_expert  https://tfhub.dev/google/collections/experts/b...  \n",
       "1   bit_expert  https://tfhub.dev/google/collections/experts/b...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bit_expert_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Latent Diffusion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [torchvision_df, taskonomy_df, timm_df, clip_df, vissl_df, \n",
    "           dino_df, midas_df, yolo_df, detectron_df, timm_pluspretrain_df]\n",
    "\n",
    "custom_df_list = [slip_df, seer_df, ipcl_df, bit_expert_df]\n",
    "\n",
    "include_custom_models = True\n",
    "if include_custom_models:\n",
    "    df_list = df_list + custom_df_list\n",
    "    \n",
    "column_order = ['model','train_type','train_data','architecture','model_class','task_cluster',\n",
    "                'model_display_name', 'description', 'model_source', 'model_source_url','weights_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_data</th>\n",
       "      <th>architecture</th>\n",
       "      <th>model_class</th>\n",
       "      <th>task_cluster</th>\n",
       "      <th>model_display_name</th>\n",
       "      <th>description</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_source_url</th>\n",
       "      <th>weights_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BiT-Expert-ResNet-V2-Arthropod</td>\n",
       "      <td>bit_expert</td>\n",
       "      <td>big_transfer</td>\n",
       "      <td>ResNet50-V2</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>Expertise</td>\n",
       "      <td>BiT-Expert-ResNet-V2-Arthropod</td>\n",
       "      <td>ResNet50-V2 trained first via ImageNet21K data...</td>\n",
       "      <td>bit_expert</td>\n",
       "      <td>https://tfhub.dev/google/collections/experts/b...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>depth_euclidean</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>Convolutional</td>\n",
       "      <td>3D</td>\n",
       "      <td>Euclidean Depth</td>\n",
       "      <td>Depth estimation</td>\n",
       "      <td>taskonomy</td>\n",
       "      <td>github.com/alexsax/midlevel-reps/tree/visualpr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ViT-L/14</td>\n",
       "      <td>clip</td>\n",
       "      <td>openai400M</td>\n",
       "      <td>ViT-L/14</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>Vision-Language</td>\n",
       "      <td>CLiP-ViT-L/14</td>\n",
       "      <td>CLiP-ViT-L/14, a hybrid vision-language model.</td>\n",
       "      <td>clip</td>\n",
       "      <td>https://github.com/openai/CLIP</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model  train_type    train_data architecture  \\\n",
       "8  BiT-Expert-ResNet-V2-Arthropod  bit_expert  big_transfer  ResNet50-V2   \n",
       "6                 depth_euclidean   taskonomy     taskonomy     resnet50   \n",
       "7                        ViT-L/14        clip    openai400M     ViT-L/14   \n",
       "\n",
       "     model_class     task_cluster              model_display_name  \\\n",
       "8  Convolutional        Expertise  BiT-Expert-ResNet-V2-Arthropod   \n",
       "6  Convolutional               3D                 Euclidean Depth   \n",
       "7    Transformer  Vision-Language                   CLiP-ViT-L/14   \n",
       "\n",
       "                                         description model_source  \\\n",
       "8  ResNet50-V2 trained first via ImageNet21K data...   bit_expert   \n",
       "6                                   Depth estimation    taskonomy   \n",
       "7     CLiP-ViT-L/14, a hybrid vision-language model.         clip   \n",
       "\n",
       "                                    model_source_url weights_url  \n",
       "8  https://tfhub.dev/google/collections/experts/b...         NaN  \n",
       "6  github.com/alexsax/midlevel-reps/tree/visualpr...         NaN  \n",
       "7                     https://github.com/openai/CLIP         NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(df_list)[column_order].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(df_list)[column_order].to_csv('model_typology.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### On the Lab Slab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision_alternative_dictlist = [\n",
    "    \n",
    "    {'model': 'deeplabv3_resnet101', 'model_display_name': 'DeepLabV3-Resnet101',\n",
    "     'train_type': 'segmentation', 'train_data': 'coco2017'},\n",
    "    \n",
    "    {'model': 'fcn_resnet101', 'model_display_name': 'FCN-Resnet101', \n",
    "     'train_type': 'segmentation', 'train_data': 'coco2017'},\n",
    "    \n",
    "    {'model': 'fasterrcnn_resnet50_fpn', 'model_display_name': 'FasterRCNN-Resnet50-FPN', \n",
    "     'train_type': 'detection', 'train_data': 'coco2017'},\n",
    "    \n",
    "    {'model': 'maskrcnn_resnet50_fpn', 'model_display_name': 'MaskRCNN-Resnet50-FPN', \n",
    "     'train_type': 'detection', 'train_data': 'coco2017'},\n",
    "    \n",
    "    {'model': 'keypointrcnn_resnet50_fpn', 'model_display_name': 'KeyPointRCNN-Resnet50-FPN', \n",
    "     'train_type': 'detection', 'train_data': 'coco2017'},\n",
    "    \n",
    "    {'model': 'r3d_18', 'model_display_name': 'R3D-18', \n",
    "     'train_type': 'video', 'train_data': 'kinetics400'},\n",
    "    \n",
    "    {'model': 'r2plus1d_18', 'model_display_name': 'R2Plus1D-18', \n",
    "     'train_type': 'video', 'train_data': 'kinetics400'},\n",
    "    \n",
    "    {'model': 'mc3_18', 'model_display_name': 'MC3-18',\n",
    "     'train_type': 'video', 'train_data': 'kinetics400'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "neptune": {
   "notebookId": "9611440e-d6a3-485f-9618-c86b1b61c939",
   "projectVersion": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
